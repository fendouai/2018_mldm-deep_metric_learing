{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Learning for Clustering Discrete Sequences\n",
    "\n",
    "* https://stackoverflow.com/questions/38260113/implementing-contrastive-loss-and-triplet-loss-in-tensorflow\n",
    "* http://scikit-learn.org/stable/modules/manifold.html\n",
    "\n",
    "# Preparation:\n",
    "* define experiment X in config/all_experiments.py\n",
    "* execute 010_generate_vocabulary.py -en X\n",
    "* execute 020_generate_training_sequences.py -en X\n",
    "* execute 025_extract_signatures.py -en X\n",
    "\n",
    "# Papers\n",
    "\n",
    "* [1] FaceNet https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\n",
    "* [2] Siamese Network: http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf\n",
    "* [3] Triplet Network: https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Wang_Learning_Fine-grained_Image_2014_CVPR_paper.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Setup notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith(\"1.4\") # the version we used\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join as jp\n",
    "import logging \n",
    "import library.helpers as h\n",
    "import library.tensorflow_helpers as tfh\n",
    "import time\n",
    "from library.vocabulary import *\n",
    "from tensorflow.contrib.tensorboard.plugins import projector # for visualizing embeddings\n",
    "import re\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "\n",
    "import matplotlib # plotting stuff\n",
    "matplotlib.use('Agg') # for displaying plots in console without display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_file=True\n",
    "# fix training\n",
    "RANDOM_SEED = 0 \n",
    "# configure numpy \n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# configure tensorflow\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "# configure logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# configure ipython display\n",
    "def show(img_file):\n",
    "    try: # only works in ipython notebook\n",
    "        display(Image(filename=img_file))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.4 Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:library.helpers:Created directory: graphs/basline-jaccard-156\n"
     ]
    }
   ],
   "source": [
    "LOG_NAME = \"bgl2\" # [unix_forensic, bgl2, spirit2, synthetic_10, synthetic_reverse_10, newsgroup20_200]\n",
    "MODEL_NAME = \"basline-jaccard\"\n",
    "DATA_DIR = \"data\"\n",
    "RESULTS_DIR = jp(\"results\", LOG_NAME, MODEL_NAME)\n",
    "h.create_dir(RESULTS_DIR)\n",
    "VIZUALIZATIONS_DIR = \"visualizations\"\n",
    "INPUTS_DIR = jp(DATA_DIR, \"inputs\")\n",
    "\n",
    "# result files\n",
    "SHARD_SIZE = 7900 # spirit2 6500\n",
    "RESULTS_FILE=jp(RESULTS_DIR, \"%0.5d-%0.2f-results.csv\")\n",
    "\n",
    "ENCODER_INPUTS_PATH = jp(DATA_DIR, \"encoder_inputs\", \"%s.idx\"%LOG_NAME)\n",
    "ENC_SEQUENCE_LENGTH_PATH = jp(DATA_DIR, \"sequence_lengths\", \"%s_enc.idx\"%LOG_NAME)\n",
    "\n",
    "RAW_LOG = jp(DATA_DIR, \"raw\", \"%s.log\"%LOG_NAME)\n",
    "LOGLINES = np.array([l[:-1] for l in list(open(RAW_LOG))])\n",
    "\n",
    "SIGNATURE_FILE =jp(DATA_DIR, \"signatures\",\"%s.sig\"%LOG_NAME)\n",
    "SIGNATURES = np.array(list(open(SIGNATURE_FILE)))\n",
    "\n",
    "h.create_dir(DATA_DIR)  # power traces go here\n",
    "h.create_dir(INPUTS_DIR)\n",
    "h.create_dir(VIZUALIZATIONS_DIR) # charts we generate\n",
    "h.create_dir(RESULTS_DIR)\n",
    "\n",
    "h.create_dir(\"graphs\") \n",
    "\n",
    "TAG_NUM = -1 # set >1 to use a specific tag\n",
    "\n",
    "if TAG_NUM < 0:\n",
    "    TAG = \"%0.3d\"%(len(os.listdir(\"graphs\"))+1)\n",
    "    DO_TRAINING = True\n",
    "else:\n",
    "    TAG = \"%0.3d\"%(TAG_NUM)\n",
    "    DO_TRAINING = False\n",
    "\n",
    "GRAPH_DIR = jp(\"graphs\", \"%s-%s\"%(MODEL_NAME, TAG))\n",
    "h.create_dir(GRAPH_DIR) # store tensorflow calc graph here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Hyper Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Max. Encoder Sequence Length: 176\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100 # 73 62 \n",
    "NUM_EPOCHS = 20\n",
    "MAX_GRADIENT_NORM = 0.5\n",
    "STATE_SIZE = 128 #  32\n",
    "test_fraction=0.1\n",
    "\n",
    "NUM_LSTM_LAYERS = 1\n",
    "ALPHA = 1.0 # distance margin \n",
    "DTYPE = tf.float32 # datatype for network parameters\n",
    "\n",
    "NUM_SEQUENCES = len(SIGNATURES)\n",
    "\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.95\n",
    "MAX_ENC_SEQ_LENGTH =  max([int(s) for s in list(open(ENC_SEQUENCE_LENGTH_PATH,\"r\"))])\n",
    "VOCABULARY = Vocabulary.load(LOG_NAME, \"\")\n",
    "\n",
    "\n",
    "TF_LEARNING_RATE = tf.Variable(0.0001, trainable=False, name=\"Learning_rate\") # alpha of our training step\n",
    "TF_KEEP_PROBABILTIY = tf.Variable(1.0, trainable=False, name=\"Dropout_keep_probability\") # can be added to feeddict\n",
    "TF_GLOBAL_STEP = tf.Variable(0, trainable=False, name=\"Global_step\") # keeps track of the current training step\n",
    "\n",
    "logger.info(\"Max. Encoder Sequence Length: %s\"%MAX_ENC_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101872"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCABULARY.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundup(x, to=100):\n",
    "    return int(math.ceil(x / to)) * to\n",
    "\n",
    "def rounddown(x,to=100):\n",
    "    return int(math.floor(x / to)) * to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data\n",
    "\n",
    "* Sequence of tokens $T$ \n",
    "* We build a vocabulary, which is a map of each unique item in the vocabulary to an integer\n",
    "\n",
    "* To generate your training / test sequences, execute scripts: 010, 020, and 025. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Parse data to Memmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder inputs shape:  (474796, 176)\n",
      "Encoder inputs(tok):  - time _ stamp short _ date node _ id _ 01 date _ time node _ id _ 01 ras kernel info instruction cache parity error corrected PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN\n",
      "Encoder inputs(int): [[101246  21091  62190   4181  17906  62190  48269  23798  62190   1799\n",
      "   62190  30581  48269  62190  21091  23798  62190   1799  62190  30581\n",
      "   54990  49276  94735  49065  62000  45014  94899  46912      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0]] Length: 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_input_line(line, max_seq_length):\n",
    "    split_line = line[:-1].split(\" \") # cut \\n at the end\n",
    "    \n",
    "    split_line_ints = [int(sl) for sl in split_line if len(sl.strip())>0] # pad sequence with zeros\n",
    "    padding  = [0] * (max_seq_length - len(split_line_ints))\n",
    "    padded_line_ints = split_line_ints +  padding\n",
    "    return np.array(padded_line_ints)\n",
    "\n",
    "def parse_input_file(input_file, output_file,  max_seq_length, force_regeneration=False, dtype=\"int32\"):\n",
    "    output_path = jp(INPUTS_DIR, output_file)\n",
    "    if not h.file_exists(output_path) or force_regeneration:\n",
    "        fp = np.memmap(output_path, dtype=dtype, mode='w+', shape=(NUM_SEQUENCES,max_seq_length))\n",
    "        # save inputs to memmap\n",
    "        for line_id, line in enumerate(list(open(input_file,\"r\"))):\n",
    "            #print(line, parse_input_line(line, max_seq_length))\n",
    "            fp[line_id,:]= parse_input_line(line, max_seq_length)\n",
    "        \n",
    "    else:\n",
    "        logger.info(output_path +\" already exists, delete it for regeneration.\")\n",
    "        fp = np.memmap(output_path, dtype=dtype, mode='r', shape=(NUM_SEQUENCES,max_seq_length))\n",
    "    return fp\n",
    "\n",
    "\n",
    "# load memmaps for seqlength (enc,dec) and (x_enc x_dec y_dec )\n",
    "ENCODER_INPUTS  = parse_input_file(ENCODER_INPUTS_PATH, \"enc_input-%s.mm\"%LOG_NAME ,  MAX_ENC_SEQ_LENGTH, force_regeneration=True)\n",
    "ENCODER_SEQLENGTH = np.array([int(s) for s in list(open(ENC_SEQUENCE_LENGTH_PATH,\"r\"))])\n",
    "SIGNATURE_FILE =jp(DATA_DIR, \"signatures\",\"%s.sig\"%LOG_NAME)\n",
    "SIGNATURES = np.array(list(open(SIGNATURE_FILE)))\n",
    "\n",
    "print(\"Encoder inputs shape: \",ENCODER_INPUTS.shape)\n",
    "print(\"Encoder inputs(tok): \", VOCABULARY.index_seq_to_line(ENCODER_INPUTS[0:1,:].flatten()))\n",
    "print(\"Encoder inputs(int):\", ENCODER_INPUTS[0:1,:], \"Length:\",  ENCODER_SEQLENGTH[0])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise Jaccard distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(s1, s2): \n",
    "    len_union = np.count_nonzero(np.union1d(s1,s2))\n",
    "    len_intersect = np.count_nonzero(np.intersect1d(s1,s2, ))\n",
    "    if len_union==0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 1- (len_intersect / len_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0.10 test fraction (0.0=all)\n",
      "num test sequences: (47300, 176)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "random_permutation = np.random.permutation(ENCODER_INPUTS.shape[0])\n",
    "\n",
    "TEST_START_INDEX = roundup(int(NUM_SEQUENCES*(1-test_fraction)))\n",
    "TEST_END_INDEX = rounddown(NUM_SEQUENCES)\n",
    "# LOGLINES = np.array(list(open(jp(DATA_DIR, \"raw\", \"%s.log\"%LOG_NAME) )))\n",
    "SIGNATURES=np.array([int(s) for s in SIGNATURES])\n",
    "\n",
    "if test_fraction>0: # if a test / train fraction is defined \n",
    "    TEST_INPUTS = ENCODER_INPUTS[random_permutation][TEST_START_INDEX:TEST_END_INDEX]\n",
    "    TEST_LABELS = SIGNATURES[random_permutation][TEST_START_INDEX:TEST_END_INDEX]\n",
    "    # LOGLINES_TEST = LOGLINES[random_permutation][TEST_START_INDEX:TEST_END_INDEX] \n",
    "else: # otherwise use whole dataset for test / train\n",
    "    0/0 \n",
    "    pass \n",
    "\n",
    "   \n",
    "print(\"Using %0.2f test fraction (0.0=all)\"%test_fraction)\n",
    "print(\"num test sequences:\", TEST_INPUTS.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation rate (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate final results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_shard(out_csv_name, pw_ji, labels_x, labels_y, d = 0.00, d_step = 0.005, d_max=1.0):\n",
    "    \n",
    "    h.save_to_csv(data_rows=[[\n",
    "        \"Distance Threshhold\",\n",
    "        \"True Positives\", \n",
    "        \"False Positives\", \n",
    "        \"True Negative\", \n",
    "        \"False Negative\", \n",
    "        \"Num True Same\", \n",
    "        \"Num True Diff\", \n",
    "    ]], outfile_name=out_csv_name, mode=\"w\")\n",
    "    \n",
    "    \n",
    "    # calculate true accepts / false accepts based on labels\n",
    "    n_labels = len(labels_x)\n",
    "    tl_row = np.repeat( np.array(labels_x).reshape((n_labels,1)), n_labels, axis=1 )\n",
    "    tl_col = np.repeat( np.array(labels_y).reshape((1,n_labels)), n_labels, axis=0 ) \n",
    "    p_same = np.equal(tl_row, tl_col).astype(\"int8\")\n",
    "    p_diff = np.not_equal(tl_row, tl_col).astype(\"int8\")\n",
    "    num_true_same = p_same.sum()\n",
    "    num_true_diff = p_diff.sum()\n",
    "    \n",
    "    while True:\n",
    "        calc_same = np.zeros((n_labels, n_labels))\n",
    "        calc_same[np.where(pw_ji<=d)]=1\n",
    "        \n",
    "        tp = np.sum(np.logical_and(calc_same, p_same))\n",
    "        fp = np.sum(np.logical_and(calc_same, np.logical_not(p_same)))\n",
    "        tn = np.sum(np.logical_and(np.logical_not(calc_same), np.logical_not(p_same)))\n",
    "        fn = np.sum(np.logical_and(np.logical_not(calc_same), p_same))\n",
    "        \n",
    "        h.save_to_csv(data_rows=[[d, tp, fp, tn, fn,num_true_same,num_true_diff]], outfile_name=out_csv_name, mode=\"a\")\n",
    "        \n",
    "        d+=d_step\n",
    "        if d>d_max:\n",
    "            break\n",
    "\n",
    "def evaluate_all_shards(inputs, labels, shard_size,shard_indizes,  results_fn, d_start=0.0, d_step=0.005, d_max=1.0 ):\n",
    "    num_test_examples = inputs.shape[0]\n",
    "    times = []\n",
    "    for i, shard_index in enumerate(shard_indizes):\n",
    "        s = time.time()\n",
    "        shard_x, shard_y = shard_index\n",
    "        out_csv_name = results_fn+\"_%0.2d-%0.2d\"%(shard_x, shard_y)\n",
    "        if os.path.exists(out_csv_name):\n",
    "            shard_data = h.load_from_csv(out_csv_name)\n",
    "            if len(shard_data) == len(np.arange(d_start, d_max, d_step))+1: # data was completely loaded, don't need to regenerate this shard\n",
    "                print(\"Shard %i exists and is complete, skipping\"%i)\n",
    "                continue\n",
    "        print(\"Current shard\", shard_index, \"%i/%i\"%(i, len(shard_indizes)))\n",
    "        start_index_x = shard_x*shard_size\n",
    "        start_index_y = shard_y*shard_size\n",
    "        end_index_x = min((shard_x+1)*shard_size, num_test_examples)\n",
    "        end_index_y = min((shard_y+1)*shard_size, num_test_examples)\n",
    "\n",
    "        # calcualte pairwise distances\n",
    "        shard_inputs_x = inputs[start_index_x:end_index_x,:]\n",
    "        shard_labels_x = labels[start_index_x:end_index_x]\n",
    "\n",
    "        shard_inputs_y = inputs[start_index_y:end_index_y,:]\n",
    "        shard_labels_y = labels[start_index_y:end_index_y]\n",
    "\n",
    "        pw_ji = pairwise_distances(shard_inputs_x,shard_inputs_y, metric=jaccard_distance, n_jobs=8) \n",
    "\n",
    "        # evaluate pairwise distances \n",
    "        \n",
    "        evaluate_shard(out_csv_name, pw_ji, shard_labels_x, shard_labels_y, d=d_start,  d_step = d_step, d_max=d_max)\n",
    "        e=time.time()\n",
    "        times.append( (e-s)/60 )\n",
    "        print(\"Avg time in min for shard: %0.2f  \"%np.mean(times))\n",
    "            \n",
    "def run_evaluation(inputs, labels, shard_size, results_fn, d_start=0.0, d_step=0.005, d_max=1.0):\n",
    "    results_fn = results_fn%(shard_size, test_fraction)\n",
    "    \n",
    "    num_test_examples = inputs.shape[0]\n",
    "    num_x = inputs.shape[0]//shard_size\n",
    "    if not num_test_examples%shard_size==0 :# need to be a square matrix\n",
    "        print(\"Allowed shard sizes\")\n",
    "        for i in range(100, num_test_examples):\n",
    "            if num_test_examples%i==0:\n",
    "                print(i)\n",
    "        0/0\n",
    "    shard_indizes = list(itertools.product(range(num_x),repeat=2))\n",
    "    num_shards = len(shard_indizes)\n",
    "    num_distances = len(list(np.arange(d_start,d_max,d_step)))\n",
    "    num_metrics = 7 \n",
    "    \n",
    "    evaluate_all_shards(inputs, labels, shard_size, shard_indizes, results_fn, d_start, d_step, d_max )\n",
    "    \n",
    "    all_data = np.ndarray(shape=(num_shards, num_distances, num_metrics), dtype=\"float32\")\n",
    "\n",
    "    for i, shard_index in enumerate(shard_indizes):\n",
    "        # load shard\n",
    "        shard_x, shard_y = shard_index\n",
    "        out_csv_name = results_fn+\"_%0.2d-%0.2d\"%(shard_x, shard_y)\n",
    "        shard_data = h.load_from_csv(out_csv_name)\n",
    "        shard_data = shard_data[1:] # cut header row \n",
    "        all_data[i] = np.array(shard_data)\n",
    "\n",
    "\n",
    "    final_data  = np.ndarray(shape=(num_distances, 10), dtype=\"float32\")\n",
    "\n",
    "    final_data[:,0] = all_data[0,:,0] # all distances (are same over all shards)\n",
    "\n",
    "    final_data[:,1] = all_data.sum(axis=0)[:,1] # True Positives\n",
    "    final_data[:,2] = all_data.sum(axis=0)[:,2] # False Positives\n",
    "    final_data[:,3] = all_data.sum(axis=0)[:,3] # True Negatives\n",
    "    final_data[:,4] = all_data.sum(axis=0)[:,4] # False Negatives\n",
    "    final_data[:,5] = all_data.sum(axis=0)[:,5] # Num true same (are same over all shards)\n",
    "    final_data[:,6] = all_data.sum(axis=0)[:,6] # Num true diff  (are same over all shards)\n",
    "\n",
    "    final_data[:,7] = final_data[:,1]/final_data[:,5] # validation rate \n",
    "    final_data[:,8] = final_data[:,2]/final_data[:,6] # false acceptance rate  \n",
    "\n",
    "    final_data[:,9] = (final_data[:,1] + final_data[:,3]) / (final_data[:,1:1+4].sum(axis=1)) \n",
    "\n",
    "    \n",
    "    h.save_to_csv(data_rows=[[\n",
    "            \"Distance Threshhold\",\n",
    "            \"True Positives\", \n",
    "            \"False Positives\", \n",
    "            \"True Negative\", \n",
    "            \"False Negative\", \n",
    "            \"Num true same\", \n",
    "            \"Num true diff\", \n",
    "            \"Validation Rate\",\n",
    "            \"False Acceptance Rate\",\n",
    "            \"Accuracy\"\n",
    "        ]], outfile_name=results_fn, mode=\"w\", convert_float=False)\n",
    "    h.save_to_csv(data_rows=final_data, outfile_name=results_fn, mode=\"a\", convert_float=True)\n",
    "\n",
    "    logger.info(\"Evaluation done, saved to '%s'\"%results_fn)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current shard (0, 0) 0/25\n",
      "Avg time in min for shard: 17.58  \n",
      "Current shard (0, 1) 1/25\n",
      "Avg time in min for shard: 17.68  \n",
      "Current shard (0, 2) 2/25\n",
      "Avg time in min for shard: 17.70  \n",
      "Current shard (0, 3) 3/25\n",
      "Avg time in min for shard: 17.71  \n",
      "Current shard (0, 4) 4/25\n",
      "Avg time in min for shard: 17.74  \n",
      "Current shard (1, 0) 5/25\n",
      "Avg time in min for shard: 17.75  \n",
      "Current shard (1, 1) 6/25\n",
      "Avg time in min for shard: 17.75  \n",
      "Current shard (1, 2) 7/25\n",
      "Avg time in min for shard: 17.77  \n",
      "Current shard (1, 3) 8/25\n",
      "Avg time in min for shard: 17.80  \n",
      "Current shard (1, 4) 9/25\n",
      "Avg time in min for shard: 17.82  \n",
      "Current shard (2, 0) 10/25\n",
      "Avg time in min for shard: 17.84  \n",
      "Current shard (2, 1) 11/25\n",
      "Avg time in min for shard: 17.86  \n",
      "Current shard (2, 2) 12/25\n",
      "Avg time in min for shard: 17.88  \n",
      "Current shard (2, 3) 13/25\n",
      "Avg time in min for shard: 17.88  \n",
      "Current shard (2, 4) 14/25\n",
      "Avg time in min for shard: 17.90  \n",
      "Current shard (3, 0) 15/25\n",
      "Avg time in min for shard: 17.90  \n",
      "Current shard (3, 1) 16/25\n",
      "Avg time in min for shard: 17.90  \n",
      "Current shard (3, 2) 17/25\n",
      "Avg time in min for shard: 17.89  \n",
      "Current shard (3, 3) 18/25\n",
      "Avg time in min for shard: 17.88  \n",
      "Current shard (3, 4) 19/25\n",
      "Avg time in min for shard: 17.87  \n",
      "Current shard (4, 0) 20/25\n",
      "Avg time in min for shard: 17.87  \n",
      "Current shard (4, 1) 21/25\n",
      "Avg time in min for shard: 17.87  \n",
      "Current shard (4, 2) 22/25\n",
      "Avg time in min for shard: 17.87  \n",
      "Current shard (4, 3) 23/25\n",
      "Avg time in min for shard: 17.87  \n",
      "Current shard (4, 4) 24/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Evaluation done, saved to 'results/bgl2/basline-jaccard/09460-0.10-results.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg time in min for shard: 17.86  \n"
     ]
    }
   ],
   "source": [
    "shards_9460 = run_evaluation(inputs=TEST_INPUTS, labels=TEST_LABELS, shard_size=9460, results_fn=RESULTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
