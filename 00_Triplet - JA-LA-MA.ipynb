{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Metric Learning for Clustering Discrete Sequences\n",
    "\n",
    "* https://stackoverflow.com/questions/38260113/implementing-contrastive-loss-and-triplet-loss-in-tensorflow\n",
    "* http://scikit-learn.org/stable/modules/manifold.html\n",
    "\n",
    "\n",
    "## Main idea\n",
    "* use jaccard distance for rough distinction\n",
    "* use labels for fine tuning \n",
    "\n",
    "\n",
    "## Preparation:\n",
    "* define experiment X in config/all_experiments.py\n",
    "* execute 010_generate_vocabulary.py -en X\n",
    "* execute 020_generate_training_sequences.py -en X\n",
    "* execute 025_extract_signatures.py -en X\n",
    "\n",
    "## Papers and resources\n",
    " \n",
    "* [1] FaceNet https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Schroff_FaceNet_A_Unified_2015_CVPR_paper.pdf\n",
    "* [2] Siamese Network: http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf\n",
    "* [3] Triplet Network: https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Wang_Learning_Fine-grained_Image_2014_CVPR_paper.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup notebook and environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith(\"1.4\") # the version we used\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join as jp\n",
    "import logging \n",
    "import library.helpers as h\n",
    "import library.tensorflow_helpers as tfh\n",
    "import time\n",
    "from library.vocabulary import *\n",
    "from tensorflow.contrib.tensorboard.plugins import projector # for visualizing embeddings\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain # chaining labeled examples\n",
    "import testing\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "\n",
    "import matplotlib # plotting stuff\n",
    "matplotlib.use('Agg') # for displaying plots in console without display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# fix training\n",
    "RANDOM_SEED = 0 \n",
    "# configure numpy \n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# configure tensorflow\n",
    "tf.set_random_seed(RANDOM_SEED)\n",
    "\n",
    "# configure logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# configure ipython display\n",
    "def show(img_file):\n",
    "    try: # only works in ipython notebook\n",
    "        display(Image(filename=img_file))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100 # 73 62 151 # for jaccard distances\n",
    "NUM_EPOCHS = 30\n",
    "JD_POS_THRESHOLD=0.30\n",
    "JD_NEG_THRESHOLD=0.70\n",
    "NUM_MAX_EXAMPLES_PER_CLASS = -1 # -1 for all \n",
    "NUM_LABELED_EXAMPLES = 1000 # -1 for all [1000, 2500, 5000]\n",
    "MAX_LABELED_LINE_TOKENS = 50 # -1 for no restriction\n",
    "test_fraction = 0.1\n",
    "\n",
    "MAX_GRADIENT_NORM = 0.5\n",
    "STATE_SIZE = 32 #  32\n",
    "EARLY_STOPPING_THRESHOLD = 0.05\n",
    "\n",
    "NUM_LSTM_LAYERS = 1\n",
    "ALPHA_JACCARD = 0.8 # distance margin \n",
    "DTYPE = tf.float32 # datatype for network parameters\n",
    "INVERTED = True \n",
    "LEARNING_RATE_DECAY_FACTOR = 0.95\n",
    "\n",
    "TF_LEARNING_RATE = tf.Variable(0.01, trainable=False, name=\"Learning_rate\") # alpha of our training step\n",
    "TF_KEEP_PROBABILTIY = tf.Variable(0.90, trainable=False, name=\"Dropout_keep_probability\") # can be added to feeddict\n",
    "TF_GLOBAL_STEP = tf.Variable(0, trainable=False, name=\"Global_step\") # keeps track of the current training step\n",
    "\n",
    "\n",
    "ADD_EXTRA_POSITIVE_EXAMPLES_PER_CLASS = 2 # \n",
    "\n",
    "LOG_NAME = \"bgl2\" # [unix_forensic, bgl2, spirit2, synthetic_10, synthetic_reverse_10]\n",
    "TAG_NUM = -1 # set >1 to use a specific tag\n",
    "SHARD_SIZE=9460  #spirit2:7150 bgl2: 9460 unix_forensic: 1050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create directories and define file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:library.helpers:Created directory: results/bgl2/jd-la-x01000-pt30-nt70-ll00050-lcall-ee00002-ep30\n",
      "INFO:library.helpers:Created directory: graphs/jd-la-x01000-pt30-nt70-ll00050-lcall-ee00002-ep30-204\n",
      "INFO:__main__:signature_extraction.extract_bgl2 module loaded\n"
     ]
    }
   ],
   "source": [
    "def fla(num):\n",
    "    if num==0:\n",
    "        return \"non\"\n",
    "    if num<0:\n",
    "        return \"all\"\n",
    "    else:\n",
    "        return \"%0.5d\"%num\n",
    "h.create_dir(\"graphs\") \n",
    "if TAG_NUM < 0:\n",
    "    TAG = \"%0.3d\"%(len(os.listdir(\"graphs\"))+1)\n",
    "    DO_TRAINING = True\n",
    "else:\n",
    "    TAG = \"%0.3d\"%(TAG_NUM)\n",
    "    DO_TRAINING = False\n",
    "    \n",
    "    \n",
    "MODEL_NAME = \"jd-la-x%s-pt%02d-nt%02d-ll%s-lc%s-ee%s-ep%0.2d\"%( fla(NUM_LABELED_EXAMPLES), \n",
    "                                                       int(JD_POS_THRESHOLD*100), \n",
    "                                                       int(JD_NEG_THRESHOLD*100), \n",
    "                                                       fla(MAX_LABELED_LINE_TOKENS), \n",
    "                                                       fla(NUM_MAX_EXAMPLES_PER_CLASS),\n",
    "                                                       fla(ADD_EXTRA_POSITIVE_EXAMPLES_PER_CLASS), \n",
    "                                                       NUM_EPOCHS\n",
    "                                                      )\n",
    "\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "RESULTS_DIR = jp(\"results\", LOG_NAME, MODEL_NAME)\n",
    "h.create_dir(RESULTS_DIR)\n",
    "RESULTS_FILE=jp(RESULTS_DIR, \"%0.5d-results.csv\")\n",
    "\n",
    "RESULTS_DIR = \"results\"\n",
    "VIZUALIZATIONS_DIR = \"visualizations\"\n",
    "INPUTS_DIR = jp(DATA_DIR, \"inputs\")\n",
    "ENCODER_INPUTS_PATH = jp(DATA_DIR, \"encoder_inputs\", \"%s.idx\"%LOG_NAME)\n",
    "ENC_SEQUENCE_LENGTH_PATH = jp(DATA_DIR, \"sequence_lengths\", \"%s_enc.idx\"%LOG_NAME)\n",
    "SIGNATURE_FILE =jp(DATA_DIR, \"signatures\",\"%s.sig\"%LOG_NAME)\n",
    "SIGNATURES = np.array(list(open(SIGNATURE_FILE)))\n",
    "\n",
    "RAW_LOG = jp(DATA_DIR, \"raw\", \"%s.log\"%LOG_NAME)\n",
    "LOGLINES = np.array([l[:-1] for l in list(open(RAW_LOG))])\n",
    "\n",
    "h.create_dir(DATA_DIR)  # power traces go here\n",
    "h.create_dir(INPUTS_DIR)\n",
    "h.create_dir(VIZUALIZATIONS_DIR) # charts we generate\n",
    "h.create_dir(RESULTS_DIR)\n",
    "\n",
    "GRAPH_DIR = jp(\"graphs\", \"%s-%s\"%(MODEL_NAME, TAG))\n",
    "h.create_dir(GRAPH_DIR) # store tensorflow calc graph here \n",
    "\n",
    "#from library.parse_arguments import *\n",
    "\n",
    "#h.import_all(\"signature_extraction.extract_%s\"%LOG_NAME , glob=globals()) # load hyperparameters\n",
    "logger.info(\"signature_extraction.extract_%s module loaded\"%LOG_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Load vocabulary, get input statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__: 474796 Sequences in dataset \n",
      "INFO:__main__: Vocabulary loaded, 101872 tokensS\n",
      "INFO:__main__: Max. Encoder Sequence Length: 176\n"
     ]
    }
   ],
   "source": [
    "NUM_SEQUENCES = len(SIGNATURES)\n",
    "MAX_ENC_SEQ_LENGTH =  max([int(s) for s in list(open(ENC_SEQUENCE_LENGTH_PATH,\"r\"))])\n",
    "VOCABULARY = Vocabulary.load(LOG_NAME, \"\")\n",
    "SIGNATURE_FILE = jp(DATA_DIR, \"signatures\",\"%s.sig\"%LOG_NAME)\n",
    "SIGNATURES = np.array(list(open(SIGNATURE_FILE))).astype(\"int32\")\n",
    "LOGLINES_TO_ENCODE = NUM_SEQUENCES\n",
    "EXAMPLES_BY_SIGNATURE_ID = {}\n",
    "for sig_id, sig in enumerate(SIGNATURES):\n",
    "    if not sig in EXAMPLES_BY_SIGNATURE_ID:\n",
    "        EXAMPLES_BY_SIGNATURE_ID[sig]=[]\n",
    "    EXAMPLES_BY_SIGNATURE_ID[sig].append(sig_id)\n",
    "    \n",
    "logger.info(\" %i Sequences in dataset \"%NUM_SEQUENCES)    \n",
    "logger.info(\" Vocabulary loaded, %i tokensS\"%VOCABULARY.size())  \n",
    "logger.info(\" Max. Encoder Sequence Length: %s\"%MAX_ENC_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "* Sequence of tokens $T$ \n",
    "* We build a vocabulary, which is a map of each unique item in the vocabulary to an integer\n",
    "\n",
    "* To generate your training / test sequences, execute scripts: 010, 020, and 025. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse data to Memmap\n",
    "\n",
    "* split line to integers\n",
    "* add padding\n",
    "* save into memmap if it does not exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder inputs shape:  (474796, 176)\n",
      "Encoder inputs(tok):  - time _ stamp short _ date node _ id _ 01 date _ time node _ id _ 01 ras kernel info instruction cache parity error corrected PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN PAD_TOKEN\n",
      "Encoder inputs(int): [[101246  21091  62190   4181  17906  62190  48269  23798  62190   1799\n",
      "   62190  30581  48269  62190  21091  23798  62190   1799  62190  30581\n",
      "   54990  49276  94735  49065  62000  45014  94899  46912      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0      0      0      0      0\n",
      "       0      0      0      0      0      0]] Length: 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def parse_input_line(line, max_seq_length):\n",
    "    split_line = line[:-1].split(\" \") # cut \\n at the end\n",
    "    split_line_ints = [int(sl) for sl in split_line] # pad sequence with zeros\n",
    "    padding  = [0] * (max_seq_length - len(split_line))\n",
    "    padded_line_ints = split_line_ints +  padding\n",
    "    return np.array(padded_line_ints)\n",
    "\n",
    "def parse_input_file(input_file, output_file,  max_seq_length, force_regeneration=False, dtype=\"int32\"):\n",
    "    output_path = jp(INPUTS_DIR, output_file)\n",
    "    if not h.file_exists(output_path) or force_regeneration:\n",
    "        fp = np.memmap(output_path, dtype=dtype, mode='w+', shape=(NUM_SEQUENCES,max_seq_length))\n",
    "        # save inputs to memmap\n",
    "        for line_id, line in enumerate(list(open(input_file,\"r\"))):\n",
    "            #print(line, parse_input_line(line, max_seq_length))\n",
    "            fp[line_id,:]= parse_input_line(line, max_seq_length)\n",
    "        \n",
    "    else:\n",
    "        logger.info(output_path +\" already exists, delete it for regeneration.\")\n",
    "        fp = np.memmap(output_path, dtype=dtype, mode='r', shape=(NUM_SEQUENCES,max_seq_length))\n",
    "    return fp\n",
    "\n",
    "\n",
    "# load memmaps for seqlength (enc,dec) and (x_enc x_dec y_dec )\n",
    "ENCODER_INPUTS  = parse_input_file(ENCODER_INPUTS_PATH, \"enc_input-%s.mm\"%LOG_NAME ,  MAX_ENC_SEQ_LENGTH, force_regeneration=True)\n",
    "ENCODER_SEQLENGTH = np.array([int(s) for s in list(open(ENC_SEQUENCE_LENGTH_PATH,\"r\"))])\n",
    "\n",
    "print(\"Encoder inputs shape: \",ENCODER_INPUTS.shape)\n",
    "print(\"Encoder inputs(tok): \", VOCABULARY.index_seq_to_line(ENCODER_INPUTS[0:1,:].flatten()))\n",
    "print(\"Encoder inputs(int):\", ENCODER_INPUTS[0:1,:], \"Length:\",  ENCODER_SEQLENGTH[0])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select labeled examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# SIGNATURES = labels by example id\n",
    "\n",
    "def select_labeled_examples(log_lines, signatures, enc_seq_lengths, max_linelinegth= MAX_LABELED_LINE_TOKENS, max_num_classes=NUM_MAX_EXAMPLES_PER_CLASS, max_num_labeled_examples=NUM_LABELED_EXAMPLES ):\n",
    "    labeled_examples = []\n",
    "    labeled_signatures = []\n",
    "    labeled_logs = []\n",
    "    labeled_examples_by_sig_id = {}\n",
    "    labeled_signatures_by_example_id = {}\n",
    "\n",
    "    for example_id, l in enumerate(log_lines):\n",
    "        if max_linelinegth>0 and enc_seq_lengths[example_id]>max_linelinegth: \n",
    "            continue # because we are interested in short loglines\n",
    "        \n",
    "        if max_num_labeled_examples>=0 and len(labeled_examples) >= max_num_labeled_examples:\n",
    "            break # because we have enough\n",
    "            \n",
    "        #if l in LABELED_LOGS: \n",
    "        #    continue # because we already have a similar line\n",
    "        \n",
    "        sig_id = signatures[example_id]  \n",
    "        if max_num_classes>0 and labeled_signatures.count(sig_id)>=max_num_classes:\n",
    "            continue # because we already have enough examples of this signature\n",
    "\n",
    "        # add example \n",
    "        labeled_examples.append(example_id)\n",
    "        labeled_signatures.append(sig_id)\n",
    "        labeled_logs.append(l)\n",
    "        if not sig_id in labeled_examples_by_sig_id:\n",
    "            labeled_examples_by_sig_id[sig_id]=[]\n",
    "        labeled_examples_by_sig_id[sig_id].append(example_id)\n",
    "        labeled_signatures_by_example_id[example_id]=sig_id \n",
    "\n",
    "        \n",
    "            \n",
    "    return labeled_examples_by_sig_id, labeled_signatures_by_example_id, labeled_logs\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data to train / test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 0.10 test fraction (0.0=all)\n",
      "num training sequences: 427400\n",
      "num test sequences: 47300\n",
      "using 1000 labeled examples\n",
      "Batch sizes: 10\n",
      "Batch sizes: 20\n",
      "Batch sizes: 25\n",
      "Batch sizes: 50\n",
      "Batch sizes: 100\n",
      "\n",
      "NOTE: Trainings data / test data gets randomly permuted, don't use SIGNATURES / LOGLINES / ENCODER_INPUTS / ENCODER_SEQLENGTH variables directly\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def roundup(x, to=100):\n",
    "    return int(math.ceil(x / to)) * to\n",
    "\n",
    "def rounddown(x,to=100):\n",
    "    return int(math.floor(x / to)) * to\n",
    "\n",
    "np.random.seed(0)\n",
    "random_permutation = np.random.permutation(ENCODER_INPUTS.shape[0])\n",
    "#if not np.array_equal(random_permutation[0:10], [3114,5863,467,3232,1328,7450,3889,2458,4216,7636]):\n",
    "#    print(\"Random permutation have to be same to be able to compare it to other methods\")\n",
    "#    assert False\n",
    "\n",
    "\n",
    "\n",
    "TEST_START_INDEX = roundup(int(NUM_SEQUENCES*(1-test_fraction)))\n",
    "TEST_END_INDEX = rounddown(NUM_SEQUENCES)\n",
    "LOGLINES = np.array(list(open(jp(DATA_DIR, \"raw\", \"%s.log\"%LOG_NAME) )))\n",
    "\n",
    "if test_fraction>0: # if a test / train fraction is defined \n",
    "    ENCODER_INPUTS_TRAIN  = ENCODER_INPUTS[random_permutation][0:TEST_START_INDEX] # all input sequences are allowed \n",
    "    ENCODER_SEQLENGTH_TRAIN = ENCODER_SEQLENGTH[random_permutation][0:TEST_START_INDEX]\n",
    "    LABELS_TRAIN = SIGNATURES[random_permutation][0:TEST_START_INDEX] # only labels that are not used in the test examples are allowed  \n",
    "    LOGLINES_TRAIN = LOGLINES[random_permutation][0:TEST_START_INDEX]\n",
    "\n",
    "    ENCODER_INPUTS_TEST  = ENCODER_INPUTS[random_permutation][TEST_START_INDEX:TEST_END_INDEX]\n",
    "    ENCODER_SEQLENGTH_TEST = ENCODER_SEQLENGTH[random_permutation][TEST_START_INDEX:TEST_END_INDEX]\n",
    "    LABELS_TEST = SIGNATURES[random_permutation][TEST_START_INDEX:TEST_END_INDEX]\n",
    "    #LOGLINES_TEST = LOGLINES[random_permutation][TEST_START_INDEX:TEST_END_INDEX] \n",
    "else: # otherwise use whole dataset for test / train\n",
    "    0/0\n",
    "   \n",
    "# select labeled examples\n",
    "labeled_examples_by_sig_id_TRAIN, labeled_signatures_by_example_id_TRAIN, labeled_logs_TRAIN = select_labeled_examples(LOGLINES_TRAIN, LABELS_TRAIN, ENCODER_SEQLENGTH_TRAIN)\n",
    "\n",
    "\n",
    "print(\"Using %0.2f test fraction (0.0=all)\"%test_fraction)\n",
    "NUM_TRAININGS_SEQUENCES = ENCODER_INPUTS_TRAIN.shape[0]\n",
    "NUM_TEST_SEQUENCES = ENCODER_INPUTS_TEST.shape[0] \n",
    "print(\"num training sequences:\",NUM_TRAININGS_SEQUENCES)\n",
    "print(\"num test sequences:\", NUM_TEST_SEQUENCES)\n",
    "print(\"using %i labeled examples\"%len(labeled_logs_TRAIN))\n",
    "STEPS_PER_EPOCH = int(NUM_TRAININGS_SEQUENCES / BATCH_SIZE)\n",
    "DECAY_EVERY_X_STEPS = STEPS_PER_EPOCH # once per epoch decay learning rate \n",
    "\n",
    "for i in range(10, 500):\n",
    "    if NUM_TRAININGS_SEQUENCES%i==0 and NUM_TEST_SEQUENCES%i==0:\n",
    "        print(\"Batch sizes:\", i)\n",
    "        \n",
    "print(\"\\nNOTE: Trainings data / test data gets randomly permuted, don't use SIGNATURES / LOGLINES / ENCODER_INPUTS / ENCODER_SEQLENGTH variables directly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " if not NUM_TEST_SEQUENCES%SHARD_SIZE==0 :# need to be a square matrix\n",
    "    print(\"Allowed shard sizes\")\n",
    "    for i in range(100, NUM_TEST_SEQUENCES):\n",
    "        if NUM_TEST_SEQUENCES%i==0:\n",
    "            print(i)\n",
    "    0/0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bgl2\n",
      "\n",
      "TRAIN\n",
      "train_classes 352\n",
      "train_class_avg_members 1214.20454545\n",
      "train_class_std_members 8791.54340333\n",
      "train_class_med_members 14.5\n",
      "train_class_min 1\n",
      "train_class_max 153554\n",
      "\n",
      "TEST\n",
      "test examples 47300\n",
      "test_classes 245\n",
      "test_class_avg_members 193.06122449\n",
      "test_class_std_members 1162.35267415\n",
      "test_class_med_members 10.0\n",
      "test_class_min 1\n",
      "test_class_max 16989\n"
     ]
    }
   ],
   "source": [
    "print(LOG_NAME)\n",
    "\n",
    "# train\n",
    "train_sigs, train_counts = np.unique(LABELS_TRAIN, return_counts=True)\n",
    "print(\"\\nTRAIN\")\n",
    "print(\"train_classes\", train_sigs.size )\n",
    "print(\"train_class_avg_members\", np.average(train_counts))\n",
    "print(\"train_class_std_members\", np.std(train_counts))\n",
    "print(\"train_class_med_members\", np.median(train_counts))\n",
    "print(\"train_class_min\", np.min(train_counts))\n",
    "print(\"train_class_max\", np.max(train_counts))\n",
    "\n",
    "# test\n",
    "test_sigs, test_counts = np.unique(LABELS_TEST, return_counts=True)\n",
    "print(\"\\nTEST\")\n",
    "print(\"test examples\", LABELS_TEST.size)\n",
    "print(\"test_classes\", test_sigs.size) \n",
    "print(\"test_class_avg_members\", np.average(test_counts))\n",
    "print(\"test_class_std_members\", np.std(test_counts))\n",
    "print(\"test_class_med_members\", np.median(test_counts))\n",
    "print(\"test_class_min\", np.min(test_counts))\n",
    "print(\"test_class_max\", np.max(test_counts))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph helper methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise Label Equality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# %load ./includes/pairwise_label_equality.py\n",
    "import tensorflow as tf\n",
    "\n",
    "def pairwise_label_equality(labels):\n",
    "    # check if labels are of correct size and type\n",
    "    batch_size = labels.shape[0]\n",
    "    assert len(labels.shape.as_list()) == 1, \"expect labels to be a 1d tensor of ints of batch_size\"\n",
    "    assert labels.dtype == tf.int32 or labels.dtype==tf.int64, \"expect labels to be a 1d tensor of ints of length batch_size\"\n",
    "\n",
    "    y_row = tf.expand_dims(labels,0) # [1,batch_size]\n",
    "    new_shape = tf.shape(tf.transpose(y_row)) # [batch_size, 1]\n",
    "    y_row_ary = tf.tile(input=y_row, multiples=new_shape ) # => [batch_size, batchtsize]\n",
    "    pw_label_equality = tf.equal(y_row_ary, tf.transpose(y_row_ary))\n",
    "    return pw_label_equality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.099s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "testing.run_tests_on(pairwise_label_equality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise euclidean distances\n",
    "\n",
    "Calculates the euclidean distances for each row vector of one matrix to each other row vector of a second matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# %load ./includes/pairwise_euclidean_distance.py\n",
    "def pairwise_euclidean_distances(\n",
    "    x1,  # x1 is a 2d tensor of dimension [nr1,b] h_t\n",
    "    x2,  # x2 is a 2d tensor of dimension [nr2,d] h_s\n",
    "    result_dtype=tf.float32\n",
    "):\n",
    "    x1 = tf.cast(x1, tf.float64) # perhaps cast to \n",
    "    x2 = tf.cast(x2, tf.float64)\n",
    "    \n",
    "    with tf.variable_scope(\"PairwiseEuclideanDistance\"):\n",
    "                \n",
    "        x1_row_norm = tf.reduce_sum(tf.pow(x1,2), axis=1, keep_dims=True) # [n_x1_rows, 1]\n",
    "        x2_row_norm = tf.reduce_sum(tf.pow(x2,2), axis=1, keep_dims=True) # [n_x2_rows, 1]\n",
    "\n",
    "        squared_distances=tf.matmul(\n",
    "            a=x1,\n",
    "            b=x2,\n",
    "            transpose_a=False,\n",
    "            transpose_b=True,\n",
    "        ) # => [n_x1_rows, n_x2_rows]\n",
    "        squared_distances = -2 * squared_distances \n",
    "        squared_distances = squared_distances + x1_row_norm # => broadcast as row vector \n",
    "        pairwise_sqrd_euclidean_distances = tf.abs(squared_distances + tf.transpose(x2_row_norm)) # => broadcast as column vector; \n",
    "        # use tf abs, because pairwise sqrd can get small negative zero values\n",
    "        #pairwise_sqrd_euclidean_distances = tf.abs(pairwise_sqrd_euclidean_distances) # because tensorflow knows -0 for very small numerical values\n",
    "        pairwise_euclidean_distances = tf.sqrt(pairwise_sqrd_euclidean_distances)        \n",
    "       \n",
    "        return tf.cast(pairwise_euclidean_distances, result_dtype), tf.cast(pairwise_sqrd_euclidean_distances, result_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise Jaccard Indices\n",
    "\n",
    "Calculates the pairwise jaccard indices for each row vector of one matrix to each row vector of another matrix. Each row consists of a an set of integer variables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Done\n"
     ]
    }
   ],
   "source": [
    "# %load ./includes/pairwise_jaccard_index.py\n",
    "# https://en.wikipedia.org/wiki/Jaccard_index\n",
    "\n",
    "\n",
    "# returns a 2d tensor of dimension [nr1,nr2]\n",
    "# where each element contains the pairwise jaccard index of the respective row vectors of x1 and x2\n",
    "# so the element result[0,0] is the pairwise jaccard index of x1[0,:] and x2[0,:], that is the first row vector of x1 and x2\n",
    "\n",
    "def pairwise_jaccard_indices(\n",
    "    x1,  # x1 is a 2d tensor of dimension [nr1,b]\n",
    "    x2,  # x2 is a 2d tensor of dimension [nr2,d]\n",
    "):\n",
    "    with tf.variable_scope(\"PairwiseJaccardIndices\"):\n",
    "        n_x1_rows = tf.shape(x1)[0] # [nr1, seq_len]\n",
    "        n_x2_rows = tf.shape(x2)[0] # [nr2, seq_len]\n",
    "        # first we create a copy for each element row of x1 for each row of x2\n",
    "        #\n",
    "        # Example: \n",
    "        # x1 has two rows, rx1_1 and rx1_2 \n",
    "        # x2 has three rows, rx2_1, rx2_2, rx2_3\n",
    "        #\n",
    "        # we want two 3d tensors, x1_tiled and x2_tiled that contain:\n",
    "        # x1_tiled: [rx1_1, rx1_1,rx1_1, rx1_2, rx1_2, rx1_2] \n",
    "        # x2_tiled: [rx2_1, rx2_2,rx2_3, rx2_1, rx2_2, rx2_3] \n",
    "        # so that we can calculate the pairwise intersection  / union between each of these elements\n",
    "        x1_expanded = tf.expand_dims(x1,1) # => [nr1,1,b] \n",
    "        x1_tiled = tf.tile( \n",
    "            input=x1_expanded, \n",
    "            multiples=[1, n_x2_rows, 1], \n",
    "        ) # => [nr1, nr2, b ]\n",
    "        # \n",
    "        x2_expanded = tf.expand_dims(x2,0) # => [1, nr2, d] \n",
    "        x2_tiled = tf.tile( \n",
    "            input=x2_expanded, \n",
    "            multiples=[n_x1_rows,1, 1]\n",
    "        )  # => => [nr1, nr2, d ]\n",
    "        \n",
    "        # new_shape = tf.shape(tf.transpose(y_row)) # [batch_size, 1]\n",
    "        # y_row_ary = tf.tile(input=y_row, multiples=new_shape ) # => [batch_size, batchtsize]\n",
    "\n",
    "        # calculate intersection \n",
    "        # we ignore zeros, because they are the padding elements   \n",
    "        sparse_intersection = tf.sets.set_intersection(x1_tiled,x2_tiled) # \n",
    "        dense_intersection = tf.sparse_tensor_to_dense(sparse_intersection) \n",
    "        len_intersection = tf.count_nonzero( \n",
    "            input_tensor=dense_intersection,\n",
    "            axis=2,\n",
    "            keep_dims=False,\n",
    "            dtype=tf.int32,\n",
    "        ) # =>  [nr1, nr2]\n",
    "        \n",
    "       \n",
    "        # calculate union\n",
    "        sparse_union = tf.sets.set_union(x1_tiled, x2_tiled ) # sparse_tensor\n",
    "        dense_union = tf.sparse_tensor_to_dense(sparse_union) # [nr1, nr2,  _ ]\n",
    "        len_union = tf.count_nonzero( \n",
    "            input_tensor=dense_union,\n",
    "            axis=2,\n",
    "            keep_dims=False,\n",
    "            dtype=tf.int32,\n",
    "        ) # => [nr1, nr2]\n",
    "\n",
    "        # get dice coefficent\n",
    "        pairwise_dice_index = (len_intersection) / (len_union) # => [nr1, nr2]\n",
    "        pairwise_jaccard_indices =  tf.cast(1 - pairwise_dice_index, tf.float32) # => [nr1, nr2]\n",
    "        return pairwise_jaccard_indices\n",
    "logger.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise Labels in Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# %load ./includes/pairwise_labels_in_batch.py\n",
    "# checks for each combination of an 1d array which labels are in the batch and which ones are not\n",
    "def pairwise_labels_in_batch(labels):\n",
    "    labels = tf.cast(labels, tf.int64) # because tf.tile has a weird behaviour that it erases values in an array\n",
    "    batch_size = tf.shape(labels)[0]\n",
    "    \n",
    "    assert len(labels.shape.as_list()) == 1, \"expect labels to be a 1d tensor of ints of batch_size\"\n",
    "\n",
    "    y_row = tf.expand_dims(labels,0) # [1, batchsize] \n",
    "    new_shape = tf.shape(tf.transpose(y_row)) # [batch_size, 1]\n",
    "    y_row_ary = tf.tile(input=y_row, multiples=new_shape ) # => [batch_size, batchtsize]\n",
    "    labels_in_batch = tf.logical_and(# IN BATCH\n",
    "        tf.greater_equal(x=y_row_ary, y=tf.zeros_like(y_row_ary, dtype=y_row_ary.dtype)), \n",
    "        tf.greater_equal(x=tf.transpose(y_row_ary), y=tf.zeros_like(y_row_ary, dtype=y_row_ary.dtype)) \n",
    "    )\n",
    "    return labels_in_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.101s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "testing.run_tests_on(pairwise_labels_in_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get batch \n",
    "\n",
    "Fetches a batch of sequences the input data and creates a feed dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def get_batch(graph_nodes, # names of the graph nodes\n",
    "              x_permutation, # index permutations (all xes)  \n",
    "              enc_inputs, # input sequences (all training sequences)\n",
    "              enc_seq_length, # input sequence lengths \n",
    "              labeled_sig_by_example_id, # training examples that are labeled example_id => signature_id \n",
    "              labeled_examples_by_signature_id, # signature_id => [example_id1, example_id2] \n",
    "              batch_size, # batch size to fetch               \n",
    "              batch_index=0, # which batch sto get\n",
    "              add_same_class_examples=0, \n",
    "              print_shapes=False):\n",
    "    \n",
    "    \n",
    "    max_seq_length = enc_inputs.shape[1]\n",
    "    \n",
    "    idx_start = batch_index*batch_size\n",
    "    idx_end = idx_start+batch_size\n",
    "    x_ids_current_batch = x_permutation[idx_start:idx_end] # row_ids\n",
    "\n",
    "    # find out which of the examples in our batch are labled  \n",
    "    labeled_examples = [] # labeled signatures \n",
    "    for example_id in x_ids_current_batch:\n",
    "        if example_id in labeled_sig_by_example_id:\n",
    "            labeled_examples.append(labeled_sig_by_example_id[example_id])\n",
    "    num_labeled_examples = len(labeled_examples)\n",
    "    \n",
    "    # define input containers \n",
    "    all_inputs = np.zeros(shape=(batch_size+add_same_class_examples*num_labeled_examples, max_seq_length), dtype=\"int32\") # [BATCH_SIZE + 2 * num_labeled_examples]\n",
    "    all_labels = np.zeros(shape=(batch_size+add_same_class_examples*num_labeled_examples), dtype=\"int32\") # [BATCH_SIZE + 2 * num_labeled_examples]\n",
    "    all_seq_length = np.zeros(shape=(batch_size+add_same_class_examples*num_labeled_examples), dtype=\"int32\") # [BATCH_SIZE + 2 * num_labeled_examples]\n",
    "    \n",
    "    # add primary batch examples + labels + seq_length \n",
    "    all_inputs[0:batch_size,:] = enc_inputs[x_ids_current_batch,:]\n",
    "    all_seq_length[0:batch_size] = enc_seq_length[x_ids_current_batch]\n",
    "    y_lb_labels = []\n",
    "    for x_id in x_ids_current_batch:\n",
    "        if x_id in labeled_sig_by_example_id.keys(): # if the example is in the labeled examples\n",
    "            y_lb_labels.append(labeled_sig_by_example_id[x_id])\n",
    "        else: # otherwise add -1 to ignore it for anchor - positive - negative calculation\n",
    "            y_lb_labels.append(-1)\n",
    "            \n",
    "    all_labels[0:batch_size] = y_lb_labels\n",
    "\n",
    "    # add extra examples of the same class to create more positive examples\n",
    "    i = 0 \n",
    "    for j in range(add_same_class_examples):\n",
    "        for sig_id in labeled_examples:\n",
    "            #print(batch_size, i)\n",
    "            rand_example_id = np.random.choice(labeled_examples_by_signature_id[sig_id]) \n",
    "\n",
    "            all_inputs[batch_size+i, :] = enc_inputs[rand_example_id]\n",
    "            all_labels[batch_size+i] = sig_id\n",
    "            all_seq_length[batch_size+i] = enc_seq_length[rand_example_id]\n",
    "            i+=1\n",
    "    \n",
    "    batch_dict = {\n",
    "        graph_nodes[\"x_jd\"]:all_inputs,  \n",
    "        graph_nodes[\"x_jd_seq_lengths\"]:all_seq_length,  \n",
    "        graph_nodes[\"y_lb_labels\"]:all_labels,  \n",
    "    }    \n",
    "    \n",
    "    if print_shapes:\n",
    "        for k,v in batch_dict.items():\n",
    "            logger.info(\"%s's shape: %s\"%(k.name, v.shape))\n",
    "    \n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'y_lb_labels': array([ 3, -1,  3], dtype=int32), 'x_jd': array([[1, 3],\n",
      "       [1, 2],\n",
      "       [1, 3]], dtype=int32), 'x_jd_seq_lengths': array([1, 2, 1], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "# test get_batch\n",
    "tb_graph_nodes = {\n",
    "    \"x_jd\":\"x_jd\",\n",
    "    \"x_jd_seq_lengths\":\"x_jd_seq_lengths\",\n",
    "    \"y_lb_labels\":\"y_lb_labels\",\n",
    "}\n",
    "\n",
    "tb_x_permutation_1 = [0,1,2,3,4,5]\n",
    "tb_enc_sequences = np.array([[1,3], [1, 2], [3,4], [5,5], [6,3], [7,4]])\n",
    "tb_enc_seqlength = np.array([1,2,2,1,1,1])\n",
    "tb_batch_size=2\n",
    "tb_labeled_sigs_by_row_id = {0:3, 2:5}\n",
    "tb_labeled_examples_by_sig_id = {3:[0], 5:[2]}\n",
    "\n",
    "\n",
    "tb_b = get_batch(\n",
    "    graph_nodes=tb_graph_nodes, \n",
    "    x_permutation=tb_x_permutation_1, \n",
    "    enc_inputs = tb_enc_sequences, \n",
    "    enc_seq_length= tb_enc_seqlength,\n",
    "    labeled_sig_by_example_id=tb_labeled_sigs_by_row_id,\n",
    "    labeled_examples_by_signature_id=tb_labeled_examples_by_sig_id,\n",
    "    batch_size = tb_batch_size, \n",
    "    batch_index = 0\n",
    "    )\n",
    "assert tb_b['y_lb_labels'][0]==3\n",
    "assert tb_b['y_lb_labels'][1]==-1\n",
    "assert tb_b['x_jd_seq_lengths'][0]==1\n",
    "assert tb_b['x_jd_seq_lengths'][1]==2\n",
    "assert np.array_equal(tb_b['x_jd'][0], [1,3])\n",
    "assert np.array_equal(tb_b['x_jd'][1], [1,2])\n",
    "assert len(tb_b['y_lb_labels'])==2\n",
    "\n",
    "tb_b = get_batch(\n",
    "    graph_nodes=tb_graph_nodes, \n",
    "    x_permutation=tb_x_permutation_1, \n",
    "    enc_inputs = tb_enc_sequences, \n",
    "    enc_seq_length= tb_enc_seqlength,\n",
    "    labeled_sig_by_example_id=tb_labeled_sigs_by_row_id,\n",
    "    labeled_examples_by_signature_id=tb_labeled_examples_by_sig_id,\n",
    "    batch_size = tb_batch_size, \n",
    "    batch_index = 1\n",
    "    )\n",
    "assert tb_b['y_lb_labels'][0]==5\n",
    "assert tb_b['y_lb_labels'][1]==-1\n",
    "assert tb_b['x_jd_seq_lengths'][0]==2\n",
    "assert tb_b['x_jd_seq_lengths'][1]==1\n",
    "assert np.array_equal(tb_b['x_jd'][0], [3,4])\n",
    "assert np.array_equal(tb_b['x_jd'][1], [5,5])\n",
    "assert len(tb_b['y_lb_labels'])==2\n",
    "\n",
    "# test for different permutation\n",
    "tb_b = get_batch(\n",
    "    graph_nodes=tb_graph_nodes, \n",
    "    x_permutation=[5,1,4,2,3,0], \n",
    "    enc_inputs = tb_enc_sequences, \n",
    "    enc_seq_length= tb_enc_seqlength,\n",
    "    labeled_sig_by_example_id=tb_labeled_sigs_by_row_id,\n",
    "    labeled_examples_by_signature_id=tb_labeled_examples_by_sig_id,\n",
    "    batch_size = tb_batch_size, \n",
    "    batch_index = 0\n",
    "    )\n",
    "assert tb_b['y_lb_labels'][0]==-1\n",
    "assert tb_b['y_lb_labels'][1]==-1\n",
    "assert tb_b['x_jd_seq_lengths'][0]==1\n",
    "assert tb_b['x_jd_seq_lengths'][1]==2\n",
    "assert np.array_equal(tb_b['x_jd'][0], [7,4])\n",
    "assert np.array_equal(tb_b['x_jd'][1], [1,2])\n",
    "assert len(tb_b['y_lb_labels'])==2\n",
    "\n",
    "# test for different permutation + add class labels \n",
    "tb_b = get_batch(\n",
    "    graph_nodes=tb_graph_nodes, \n",
    "    x_permutation=[5,1,4,2,3,0], # examples \n",
    "    enc_inputs = tb_enc_sequences, \n",
    "    enc_seq_length= tb_enc_seqlength,\n",
    "    labeled_sig_by_example_id=tb_labeled_sigs_by_row_id,\n",
    "    labeled_examples_by_signature_id=tb_labeled_examples_by_sig_id,\n",
    "    batch_size = tb_batch_size, \n",
    "    add_same_class_examples = 1, \n",
    "    batch_index = 0\n",
    "    )\n",
    "assert len(tb_b['y_lb_labels'])==2 # should be two because non of the examples in the batch are labeled\n",
    "\n",
    "tb_b = get_batch(\n",
    "    graph_nodes=tb_graph_nodes, \n",
    "    x_permutation=tb_x_permutation_1, \n",
    "    enc_inputs = tb_enc_sequences, \n",
    "    enc_seq_length= tb_enc_seqlength,\n",
    "    labeled_sig_by_example_id=tb_labeled_sigs_by_row_id,\n",
    "    labeled_examples_by_signature_id=tb_labeled_examples_by_sig_id,\n",
    "    batch_size = tb_batch_size,\n",
    "    add_same_class_examples = 1, \n",
    "    batch_index = 0\n",
    "    )\n",
    "\n",
    "assert len(tb_b['y_lb_labels'])==3\n",
    "assert tb_b['y_lb_labels'][0]==3\n",
    "assert tb_b['y_lb_labels'][1]==-1\n",
    "assert tb_b['y_lb_labels'][2]==3\n",
    "assert np.array_equal(tb_b['x_jd'][2], [1,3])\n",
    "assert tb_b['x_jd_seq_lengths'][2]==1\n",
    "\n",
    "print(tb_b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input, Targets \n",
    "\n",
    "The input to our model are sequences of integer number. Each integer number denotes the id a certain word has in the vocabulary. The sequences of integers are zero padded in the training. For that, we add the sequence langth to \n",
    "\n",
    "We embedd this integers in a dense embedding matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "graph_input_nodes = {} # store nodes that we want to use in our batch\n",
    "\n",
    "# inputs\n",
    "with tf.variable_scope(\"Word_embeddings\"):\n",
    "    TOKEN_EMBEDDINGS = tf.get_variable(\n",
    "        name=\"word_embeddings\",\n",
    "        shape=[VOCABULARY.size(), STATE_SIZE],\n",
    "        dtype=DTYPE,\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.5),\n",
    "        regularizer=None,\n",
    "        trainable=True,\n",
    "    ) # each row is a dense vector for each word.\n",
    "\n",
    "# Encoder Inputs\n",
    "with tf.variable_scope(\"Encoder_Inputs\"):\n",
    "    x_jd = tf.placeholder(tf.int64, [None, None], \"jd_x\") # encoder inputs loglines [BATCH_SIZE, num_steps]\n",
    "    x_jd_seq_lengths = tf.placeholder(tf.int64, [None], \"sequence_lengths\") # [BATCH_SIZE]\n",
    "    graph_input_nodes[\"x_jd\"] = x_jd\n",
    "    graph_input_nodes[\"x_jd_seq_lengths\"] = x_jd_seq_lengths\n",
    "    \n",
    "    y_lb_labels = tf.placeholder(tf.int64, [None], \"y\") # signature for each loglines\n",
    "    graph_input_nodes[\"y_lb_labels\"] = y_lb_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Encoder\n",
    "\n",
    "Our encoding network. Just a plain LSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# %load ./includes/lstm_encode.py\n",
    "# Encoder\n",
    "def LSTMEncode(input_x, input_sequences_length, scope, name=\"\"):\n",
    "    input_sequences = tf.nn.embedding_lookup(TOKEN_EMBEDDINGS, input_x) # [BATCH_SIZE, max_time, embedding_size]\n",
    "    \n",
    "    encoder_cell = tf.contrib.rnn.LSTMCell(num_units=STATE_SIZE, state_is_tuple=True)\n",
    "    encoder_cell = tf.contrib.rnn.DropoutWrapper(cell=encoder_cell,\n",
    "                                         output_keep_prob=TF_KEEP_PROBABILTIY,\n",
    "                                         input_keep_prob=TF_KEEP_PROBABILTIY,\n",
    "                                         state_keep_prob=TF_KEEP_PROBABILTIY,\n",
    "                                         dtype=DTYPE)\n",
    "    encoder_cell = tf.contrib.rnn.MultiRNNCell(cells=[encoder_cell] * NUM_LSTM_LAYERS, state_is_tuple=True)\n",
    "\n",
    "    encoder_outputs, last_encoder_state = tf.nn.dynamic_rnn(\n",
    "        cell=encoder_cell,\n",
    "        dtype=DTYPE,\n",
    "        sequence_length=input_sequences_length,\n",
    "        inputs=input_sequences,\n",
    "        scope=scope\n",
    "        )\n",
    "    last_c, last_h = last_encoder_state[0] # h is hidden state, c = memory state https://arxiv.org/pdf/1409.2329.pdf\n",
    "    z = tf.nn.l2_normalize(x=last_c, dim=1, epsilon=1e-12, name=\"OutputNormalization\")   \n",
    "    return z     \n",
    "\n",
    "with tf.variable_scope(\"Encode_Inputs\") as encode_scope:\n",
    "    z_jd = LSTMEncode(x_jd, x_jd_seq_lengths, encode_scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode loglines\n",
    "\n",
    "Operation for encdoing all loglines into an embedding. We use this embedding to visualize it in tensorboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def encode_loglines(enc_loglines, enc_seq_length, loglines, signatures, x_permutation, name=\"Batch1\", batch_size=BATCH_SIZE):\n",
    "    num_log_lines = int(loglines.shape[0]/batch_size)*batch_size # truncate batch to modulo zero of batch\n",
    "    # define graph\n",
    "    with tf.variable_scope(\"LogLineEncodings\"):\n",
    "        encoded_loglines = tf.get_variable(name=name, shape=[num_log_lines, STATE_SIZE], dtype=tf.float32, initializer=tf.zeros_initializer(), trainable=False ) \n",
    "        tf_batch_idx = tf.Variable(0, trainable=False)\n",
    "        update_indices = tf.range(tf_batch_idx*batch_size, tf_batch_idx*batch_size+batch_size)\n",
    "        update_indices = tf.reshape(update_indices, shape = [-1, 1])\n",
    "        encode_loglines_op = tf.scatter_nd_update(ref=encoded_loglines, indices=update_indices, updates=z_jd, name=\"encode_loglines_op\")\n",
    "        num_batches = int(num_log_lines / batch_size)\n",
    "    \n",
    "    tfh.initialize_unitialized_variables(session)\n",
    "    \n",
    "    for current_batch in range(0,num_batches):\n",
    "        if current_batch%10==0:\n",
    "            print(\"(%i/%i) batches encoded\"%(current_batch, num_batches))\n",
    "        # get batch\n",
    "        feed_dict  = get_batch(\n",
    "            graph_input_nodes, \n",
    "            x_permutation,  \n",
    "            batch_index=current_batch, \n",
    "            enc_inputs=enc_loglines, \n",
    "            enc_seq_length=enc_seq_length, \n",
    "            batch_size=batch_size, \n",
    "            labeled_sig_by_example_id={}, \n",
    "            labeled_examples_by_signature_id={},\n",
    "            add_same_class_examples=0\n",
    "        )\n",
    "        feed_dict[tf_batch_idx]=current_batch\n",
    "        feed_dict[TF_KEEP_PROBABILTIY]=1.0\n",
    "        # update slice in encoded lines\n",
    "        _, rui = session.run([encode_loglines_op, update_indices], feed_dict=feed_dict)\n",
    "        session.run(TF_GLOBAL_STEP.assign_add(1))\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables()) # Saver\n",
    "    saver.save(session, jp(GRAPH_DIR, MODEL_NAME), global_step=TF_GLOBAL_STEP)\n",
    "        \n",
    "    logger.info(\"%i log lines embedded into %i dimensional space (%s).\"%(num_log_lines, STATE_SIZE, name))\n",
    "\n",
    "    # write metadata\n",
    "    LOGLINE_EMBEDDINGS_METADATA_FN = '%s-%s-%s.tsv'%(LOG_NAME, \"logline_embeddings\",name)\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    with open( jp(GRAPH_DIR, LOGLINE_EMBEDDINGS_METADATA_FN) , \"w\") as mdf:\n",
    "        mdf.write(\"%s\\t%s\\t%s\\n\"%(\"ProcessId\", \"SignatureId\", \"LogLine\"))\n",
    "        for line_id, line in enumerate(loglines):\n",
    "            if line_id>=num_log_lines:\n",
    "                break\n",
    "            \n",
    "            process_id = line.split(\"]\")[0][1:].split(\" \")[0]\n",
    "            sanitized_line = re.sub(r'[^\\x00-\\x7F]+',' ', line[:-1]) # replace all non ascii characters with a space\n",
    "\n",
    "            line = \"%s\\t%s\\t%s\"%(process_id,int(signatures[line_id]), sanitized_line)+\"\\n\"\n",
    "\n",
    "            mdf.write(line)\n",
    "            labels.append(int(signatures[line_id]))\n",
    "\n",
    "    link_embedding_to_metadata(\n",
    "        embedding_var=encoded_loglines, \n",
    "        metadata_file=LOGLINE_EMBEDDINGS_METADATA_FN\n",
    "    )\n",
    "    return labels, encoded_loglines # labels are signature ids \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function / objective\n",
    "\n",
    "Intuitively, we are minimizing the distance in differences of the positive and the negative example. That is, want the positive examples to be closer to each other and the negative example farther apart of each other. \n",
    "\n",
    "Given our encoding function $e$, an anchor example $x_i^a$, a postive example $x_i^p$, a negative example $x_i^n$ and a margin $\\alpha$\", we minimize the following loss per trainingsexample $i$ of our batch:\n",
    "\n",
    "$$\n",
    "l_i = max\\left(0,\\|e(x_i^a)-e(x_i^p)\\|_2^2 - \\|e(x_i^a)-e(x_i^n)\\|_2^2 + \\alpha\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "*  Reference: [1], equation (3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "## get anchor positive examples\n",
    "\n",
    "Each row in the batch tensor is treated as anchor example. We want to calculate whether the other examples in this batch are positive to each anchor example. \n",
    "\n",
    "To determine whether the other examples are positive, we use two factors: label_equality if we both examples are labeled and jaccard distance of the sequence if one of the two examples is not labeled. \n",
    "\n",
    "To calculate the positive examples we use pairwise comparison from each row to each other element in the batch. This usually leads to matrices of dimensions [batch_size, batch_size], where and element $x_{ij}$ of such a matrix means the value of $x_i$ compared to $x_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:anchor_positive_examples defined.\n"
     ]
    }
   ],
   "source": [
    "# %load ./includes/anchor_positive_examples.py\n",
    "# returns all euclidean distances for which either the labels are the same or the jaccard distance is below the positive threshold\n",
    "def anchor_positive_examples(pw_label_equality, labels_in_batch, pw_jaccard_distances, pw_euclidean_distances, jd_pos_threshold):\n",
    "    batch_size = tf.shape(pw_label_equality)[0]\n",
    "    labels_not_in_batch = tf.logical_not(labels_in_batch) # labels in batch is a bad name. It should be - we have labeled exampled for this example\n",
    "    \n",
    "    # positive conditions\n",
    "    labels_match = tf.not_equal(pw_label_equality, tf.eye(batch_size, dtype=tf.int32)) # exclude equality between same elements\n",
    "    pw_ji_for_pos = tf.add(pw_jaccard_distances, tf.eye(batch_size)*1.5) # jaccard distance is between 0 and 1 - exclude equality between same elements\n",
    "    sequences_have_pos_jd = tf.less(x=pw_ji_for_pos, y=jd_pos_threshold, name=\"jd_pos_cond\") # sequences are\n",
    "    \n",
    "    # it's either an anchor-positive example because the jaccard distance is smaller than the threshold or because the labels are the same. \n",
    "    pos_because_of_labels = tf.logical_and(labels_in_batch, labels_match)\n",
    "    pos_because_of_jd  = tf.logical_and(labels_not_in_batch, sequences_have_pos_jd)\n",
    "    pos_cond = tf.logical_or(pos_because_of_labels, pos_because_of_jd)\n",
    "    \n",
    "    # exclude example itself from positive / negative  - euclidean distance to between two identical vectors should always be 0\n",
    "    positive_ed = tf.where(condition=pos_cond , x=pw_euclidean_distances, y=tf.ones_like(pw_euclidean_distances)*-1) # -1 means non positive\n",
    "    return positive_ed, pos_because_of_labels,  pos_because_of_jd\n",
    "logger.info(\"anchor_positive_examples defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..........\n",
      "----------------------------------------------------------------------\n",
      "Ran 10 tests in 0.796s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "testing.run_tests_on(anchor_positive_examples)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get anchor negative examples\n",
    "\n",
    "Same as anchor positive examples, only the other case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:anchor_negative_examples defined.\n"
     ]
    }
   ],
   "source": [
    "# %load ./includes/anchor_negative_examples.py\n",
    "def anchor_negative_examples(pw_label_equality, labels_in_batch, pw_jaccard_distances, pw_euclidean_distances, jd_neg_threshold):\n",
    "    batch_size = tf.shape(pw_label_equality)[0]\n",
    "    labels_not_in_batch = tf.logical_not(labels_in_batch)\n",
    "    \n",
    "    # make sure to exclude jaccard distances of the diagonal, because elements to itself are never negative\n",
    "    pw_ji_for_neg = tf.add(pw_jaccard_distances,  tf.eye(batch_size)*-1.0)\n",
    "    \n",
    "    # negative condition\n",
    "    labels_dont_match = tf.equal(pw_label_equality, tf.zeros_like(pw_label_equality, dtype=tf.int32),  name=\"la_neg_cond\")\n",
    "    sequences_have_neg_jd = tf.greater_equal(x=pw_ji_for_neg, y=jd_neg_threshold, name=\"jd_neg_cond\") # elements at the diagonal should aways have zero, so \n",
    "    \n",
    "    neg_because_of_labels = tf.logical_and(labels_in_batch, labels_dont_match) # all labels that are not equal to 1\n",
    "    neg_because_of_jd = tf.logical_and(labels_not_in_batch, sequences_have_neg_jd)  \n",
    "    neg_cond = tf.logical_or(neg_because_of_labels, neg_because_of_jd) # it's either negative because the jaccard distance is over the threshold or the labels are not matching                                           \n",
    "    \n",
    "    negative_ed = tf.where(condition=neg_cond , x=pw_euclidean_distances, y=tf.ones_like(pw_euclidean_distances)*-1)\n",
    "    return negative_ed, neg_because_of_labels, neg_because_of_jd\n",
    "\n",
    "logger.info(\"anchor_negative_examples defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "............\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 1.275s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "testing.run_tests_on(anchor_negative_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get pairwise jaccard indizes for training sequences\n",
    "\n",
    "Sørensen's original formula:\n",
    "$$ \n",
    "    Q_{søresen} = \\frac{2 * | X \\cap Y |}{| X |+| Y |}\n",
    "$$ \n",
    "with $ |X| $ being the length of the set. In our situation, we treat each token of our sequence es element of the set. \n",
    "\n",
    "We use the Jaccard index because we want to have a distance that we want to minimze instead. The dice coefficent is 0 for totally non overlapping to 1 for identical strings, whereas the jaccard is 0 if two sequences are identicall, and 1 if they are completely different.  \n",
    "\n",
    "$$\n",
    "    Q_{jaccard} = 1 - Q_{søresen}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# %load ./includes/get_positive_and_anchor_examples.py\n",
    "# https://stackoverflow.com/questions/41806689/tensorflow-get-indices-of-array-rows-which-are-zero\n",
    "# get pairwise jaccard indizes for all samples in the batch\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Get all anchor-positive / anchor-negative example permuations in  batch\n",
    "\"\"\"\n",
    "def get_positive_and_anchor_examples(\n",
    "                                    input_x, # input sequences \n",
    "                                    input_z, # encoded input sequences in embedding space z\n",
    "                                    labels_y, # labels for input sequences\n",
    "                                    jd_pos_threshold=JD_POS_THRESHOLD, jd_neg_threshold=JD_NEG_THRESHOLD):\n",
    "    # get pairwise jaccard indices for input sequences\n",
    "    batch_size = tf.shape(input_x)[0]\n",
    "    \n",
    "    # get pairwise jaccard distances for input sequences\n",
    "    pw_ji = pairwise_jaccard_indices(x1=input_x, x2=input_x)  # => [BATCH_SIZE, BATCH_SIZE]\n",
    "  \n",
    "    \n",
    "    # get pairwise euclidean distances for encoded input sequneces\n",
    "    _ , pw_sq_ed = pairwise_euclidean_distances(x1=input_z, x2=input_z)  # => [BATCH_SIZE, BATCH_SIZE]\n",
    "\n",
    "    # get pairwise label equality\n",
    "    pw_lbl_eq = pairwise_label_equality(labels_y)\n",
    "    pw_lbl_eq = tf.cast(pw_lbl_eq, tf.int32) # => [BATCH_SIZE, BATCH_SIZE] \n",
    "        \n",
    "    # check whether labels are in batch\n",
    "    labels_in_batch = pairwise_labels_in_batch(labels_y)\n",
    "   \n",
    "    # get anchor-positive examples and anchor-negative examples\n",
    "    anchor_positive_ed, pos_because_of_labels, pos_because_of_jd = anchor_positive_examples(pw_lbl_eq, labels_in_batch, pw_ji, pw_sq_ed, jd_pos_threshold)\n",
    "    anchor_negative_ed, neg_because_of_labels, neg_because_of_jd = anchor_negative_examples(pw_lbl_eq, labels_in_batch, pw_ji, pw_sq_ed, jd_neg_threshold)\n",
    "\n",
    "    # get all combinations between a=>p, a=>n for each row \n",
    "    # example: assume positive_ed = [[a,b], [c,d]] and negative_ed = [[e,f],[g,h]]\n",
    "    # then pos_row =\n",
    "    # [\n",
    "    #   [a,a],\n",
    "    #   [b,b],\n",
    "    #   [c,c],\n",
    "    #   [d,d]\n",
    "    # ]\n",
    "    # and neg_col = \n",
    "    # [\n",
    "    #   [e,f],\n",
    "    #   [e,f],\n",
    "    #   [g,h],\n",
    "    #   [g,h]\n",
    "    # ]\n",
    "    # \n",
    "    # if you use this arrays for comparison, you will have all possible combinations within one row. \n",
    "    # if you calculate the distance between pos_row and neg_col, you will get:\n",
    "    # [\n",
    "    #   [a-e, a-f], \n",
    "    #   [b-e, b-f], \n",
    "    # \n",
    "    #  ...\n",
    "    # ]   \n",
    "    pos_row = tf.tile(tf.reshape(anchor_positive_ed, [-1, 1]), [1, batch_size])\n",
    "    neg_col = tf.reshape(tf.tile(anchor_negative_ed, [1 , batch_size]), [-1, batch_size])\n",
    "    \n",
    "    # get statistics on how many examples where anchor-positive and how many were anchor negative\n",
    "    num_neg_la = tf.reduce_sum(tf.cast(neg_because_of_labels, dtype=tf.int32)) \n",
    "    num_neg_jd = tf.reduce_sum(tf.cast(neg_because_of_jd, dtype=tf.int32))\n",
    "    num_pos_la = tf.reduce_sum(tf.cast(pos_because_of_labels, dtype=tf.int32)) \n",
    "    num_pos_jd = tf.reduce_sum(tf.cast(pos_because_of_jd, dtype=tf.int32)) \n",
    "    stats = (num_neg_la, num_neg_jd, num_pos_la, num_pos_jd)\n",
    "    \n",
    "    return pos_row, neg_col, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import unittest\n",
    "from unittest import *\n",
    "\n",
    "x1 = np.array([\n",
    "    [1,2,3,4],\n",
    "    [4,5,6,1],\n",
    "    [8,2,3,4]\n",
    "])\n",
    "\n",
    "z1 = np.array([\n",
    "    [0.2,0.4],\n",
    "    [0.8,0.2],\n",
    "    [0.3,0.3],\n",
    "])\n",
    "\n",
    "y1 = np.array([1,2,1])\n",
    "\n",
    "\n",
    "def run_tests_on(get_positive_and_anchor_examples):\n",
    "    \n",
    "    class TestGetPositiveAndAnchorExamplesJDLA(TestCase):\n",
    "        def setUp(self):\n",
    "            self.input_x = tf.placeholder(shape=[None, None], dtype=tf.int32) # [batch_size, max_seq_length]\n",
    "            self.input_z = tf.placeholder(shape=[None, None], dtype=tf.int32) # [batch_size, embedding_dim]\n",
    "            self.labels_y = tf.placeholder(shape=[None], dtype=tf.int32) # [batch_size,1]\n",
    "            self.jd_pos_threshold = tf.placeholder(shape=(), dtype=tf.float32)\n",
    "            self.jd_neg_threshold = tf.placeholder(shape=(), dtype=tf.float32)\n",
    "            \n",
    "            self.op = get_positive_and_anchor_examples(\n",
    "                input_x=self.input_x, \n",
    "                input_z=self.input_z, \n",
    "                labels_y=self.labels_y,\n",
    "                jd_pos_threshold=self.jd_pos_threshold,\n",
    "                jd_neg_threshold=self.jd_neg_threshold\n",
    "            )\n",
    "\n",
    "            self.session = tf.Session()\n",
    "            self.session.run(tf.global_variables_initializer())\n",
    "            \n",
    "        def run_op(self, x,z,y,pt,nt):#shortcut\n",
    "            return self.session.run(self.op,feed_dict={\n",
    "                self.input_x:x, \n",
    "                self.input_z:z, \n",
    "                self.labels_y:y, \n",
    "                self.jd_pos_threshold:pt, \n",
    "                self.jd_neg_threshold:nt\n",
    "            })\n",
    "        \n",
    "\n",
    "        def test_shapes(self):\n",
    "            batch_size = x1.shape[0]\n",
    "            pos_row, neg_col, stats =  self.run_op(x1,z1,y1, 0.2, 0.8)\n",
    "            \n",
    "            self.assertEqual(pos_row.shape[0], batch_size * batch_size) \n",
    "            self.assertEqual(pos_row.shape[1], batch_size)\n",
    "            self.assertEqual(neg_col.shape[0], batch_size * batch_size)\n",
    "            self.assertEqual(neg_col.shape[1], batch_size)\n",
    "        \n",
    "        def test_pos_row_value_equality(self):\n",
    "            batch_size = x1.shape[0]\n",
    "            pos_row, neg_col, stats =  self.run_op(x1,z1,y1, 0.2, 0.8)\n",
    "            \n",
    "            for row_id in range(batch_size*batch_size):\n",
    "                for col_id in range(batch_size):\n",
    "                    self.assertEqual(pos_row[row_id][0], pos_row[row_id][col_id])\n",
    "        \n",
    "        def test_neg_col_value_equality(self):\n",
    "            batch_size = x1.shape[0]\n",
    "            pos_row, neg_col, stats =  self.run_op(x1,z1,y1, 0.2, 0.8)\n",
    "            for row_id in range(batch_size*batch_size):\n",
    "                for col_id in range(batch_size):\n",
    "                    self.assertEqual(neg_col[int(row_id/batch_size)*batch_size][col_id], neg_col[row_id][col_id])\n",
    "                \n",
    "\n",
    "        def tearDown(self):\n",
    "            self.session.close()\n",
    "               \n",
    "    suite = unittest.TestSuite()\n",
    "    suite.addTests(unittest.TestLoader().loadTestsFromModule(TestGetPositiveAndAnchorExamplesJDLA()))\n",
    "    unittest.TextTestRunner().run(suite)  \n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.493s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# TODO move to tests\n",
    "run_tests_on(get_positive_and_anchor_examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## permutation hinge loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Done.\n"
     ]
    }
   ],
   "source": [
    "def permutation_hinge_loss(pos_anchor_dist, neg_anchor_dist, alpha=ALPHA_JACCARD, use_mean_loss=False):\n",
    "    # pos_anchor_dist = [batch_size * batchsize, batch_size]  \n",
    "    batch_size=tf.shape(pos_anchor_dist)[1]\n",
    "    \n",
    "    # condition: exclude all invalid examples\n",
    "    # we want: distance a=>n should be larger than the distance a=>p+margin\n",
    "    # we want to catch examples where the distance a=>n-margin is smaller than the distance of the positive anchors\n",
    "    neg_greater_zero = tf.greater_equal(neg_anchor_dist, tf.zeros_like(neg_anchor_dist)) # all the valid negative examples\n",
    "    pos_greater_zero = tf.greater_equal(pos_anchor_dist, tf.zeros_like(pos_anchor_dist)) # permuted with all valid positive ones\n",
    "    d_pos_less_than_d_neg = tf.less(x=neg_anchor_dist-alpha, y=pos_anchor_dist, name=\"constraint_cond\") # which violate distance anchor-positive <  anchor-negative\n",
    "    \n",
    "    # loss calculation for all permutations\n",
    "    loss = tf.maximum(pos_anchor_dist-neg_anchor_dist+alpha, 0, \"hinge_loss\") # loss is small if  pos_anchor_dist is large and neg_anchor dist is small \n",
    "    permuations_loss  = tf.where(tf.logical_and(tf.logical_and(neg_greater_zero,pos_greater_zero),d_pos_less_than_d_neg),loss,  tf.zeros_like(pos_anchor_dist))\n",
    "    # => shape [BATCH_SIZE*BATCH_SIZE, BATCH_SIZE]. This shape is because we only want all possible combination per row of the batch\n",
    "\n",
    "    num_non_zero_perms = tf.reduce_sum(tf.cast(tf.greater(x=permuations_loss, y=tf.zeros_like(permuations_loss)), tf.float32))\n",
    "    mean_permutation_loss = tf.reduce_sum(permuations_loss , axis=1) / tf.maximum(num_non_zero_perms, 1)  # only calculate mean between non-zero calculation losses, because 0 means invalid\n",
    "    # => shape [BATCH_SIZE*BATCH_SIZE,1]\n",
    "\n",
    "    per_example_loss = tf.reshape(mean_permutation_loss, [batch_size,batch_size]) # all valid permutations per example\n",
    "    num_exp_loss_greater_zero = tf.reduce_sum(tf.cast(tf.greater(x=per_example_loss, y=tf.zeros_like(per_example_loss)), tf.float32))\n",
    "    \n",
    "    if use_mean_loss:\n",
    "        return tf.reduce_sum(per_example_loss) / num_exp_loss_greater_zero, num_non_zero_perms\n",
    "    else:\n",
    "        return tf.reduce_sum(per_example_loss), num_non_zero_perms\n",
    "logger.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:done\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"jd_la_ma_hinge_loss\"):\n",
    "    jd_pos_anchor_dist, jd_neg_anchor_dist, loss_stats = get_positive_and_anchor_examples(input_x=x_jd, input_z=z_jd, labels_y=y_lb_labels)\n",
    "    jd_la_ma_hinge_loss, num_non_zero_perms_op = permutation_hinge_loss(jd_pos_anchor_dist, jd_neg_anchor_dist, alpha=ALPHA_JACCARD)\n",
    "logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:library.tensorflow_helpers:Operations [['jd_la_ma_hinge_loss/Sum_7:0']] have at least 1 gradient for at least 1 parameter\n"
     ]
    }
   ],
   "source": [
    "tfh.ensure_gradient_flow(operations=[jd_la_ma_hinge_loss]) \n",
    "total_loss_op = jd_la_ma_hinge_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization procedure\n",
    "\n",
    "The gradient descent algorithm. Get the gradients, multiply it with the learning rate and apply it to the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Training_Step\"):\n",
    "    # adjust learning rate operation\n",
    "    tf_learning_rate = tf.train.exponential_decay(\n",
    "      learning_rate= TF_LEARNING_RATE,\n",
    "      global_step=TF_GLOBAL_STEP,\n",
    "      decay_steps=DECAY_EVERY_X_STEPS, \n",
    "      decay_rate= LEARNING_RATE_DECAY_FACTOR,\n",
    "      staircase=True)\n",
    "\n",
    "    # get gradients for all trainable parameters with respect to our loss funciton\n",
    "    tf_params = tf.trainable_variables()\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=tf_learning_rate)\n",
    "    gradients = tf.gradients(total_loss_op, tf_params)\n",
    "    \n",
    "    # apply gradient clip\n",
    "    clipped_gradients, gradient_norm = tf.clip_by_global_norm(gradients,MAX_GRADIENT_NORM)\n",
    "\n",
    "    # Update operation\n",
    "    training_step_op  = optimizer.apply_gradients(zip(clipped_gradients, tf_params), global_step=TF_GLOBAL_STEP) # learing rate decay is calulated based on this step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Done\n"
     ]
    }
   ],
   "source": [
    "# keep track of loss over time\n",
    "tf.summary.scalar(\"hinge_loss\",tf.cast(jd_la_ma_hinge_loss , tf.float32)) # summary for loss\n",
    "tf.summary.scalar('total_loss', tf.cast(total_loss_op, tf.float32))\n",
    "\n",
    "# examples in batch\n",
    "tf.summary.scalar(\"num_non_zero_perms\", tf.cast(num_non_zero_perms_op, tf.float32))\n",
    "\n",
    "# num examples jaccard distance\n",
    "tf.summary.scalar(\"num_la_neg_examples\",tf.cast(loss_stats[0], tf.float32)) # summary for loss\n",
    "tf.summary.scalar('num_jd_neg_examples', tf.cast(loss_stats[1], tf.float32))\n",
    "tf.summary.scalar(\"num_la_pos_examples\",tf.cast(loss_stats[2], tf.float32)) # summary for loss\n",
    "tf.summary.scalar('num_jd_pos_examples', tf.cast(loss_stats[3], tf.float32))\n",
    "\n",
    "\n",
    "# historgrams for all trainable variables\n",
    "for tv in tf.trainable_variables():\n",
    "    tf.summary.histogram(tv.name.replace(\":\",\"_\"),tv)  # summary for loss\n",
    "  \n",
    " \n",
    "WORD_EMBEDDINGS_METADATA_FN = '%s-%s.tsv'%(LOG_NAME, \"token_embeddings\")\n",
    "with open( jp(GRAPH_DIR, WORD_EMBEDDINGS_METADATA_FN) , \"w\") as mdf:\n",
    "    for idx in sorted([int(k) for k in VOCABULARY.index_to_token.keys()]):\n",
    "         mdf.write(\"%s\\n\"%(VOCABULARY.idx_to_token(idx)))\n",
    "logger.info(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write metadata for token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "def link_embedding_to_metadata(embedding_var, metadata_file):\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_var.name.replace(\":0\",\"\")\n",
    "    embedding.metadata_path = metadata_file\n",
    "    summary_writer = tf.summary.FileWriter(GRAPH_DIR)\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "link_embedding_to_metadata(TOKEN_EMBEDDINGS, WORD_EMBEDDINGS_METADATA_FN )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Tensorflow session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Vocabulary Size: 101872, max. sequence length(enc): 176\n",
      "INFO:__main__:Trainings examples: 427400,  batch size: 100\n",
      "INFO:__main__:Epochs: 30, batches per epoch:4274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "#logger.info(\"Random Seed: %0.3f\"%session.run(tf.random_normal([1], mean=-1, stddev=4, seed=RANDOM_SEED))[0])\n",
    "\n",
    "# summaries for training\n",
    "all_summaries_op = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(GRAPH_DIR)\n",
    "summary_writer.add_graph(session.graph)\n",
    "\n",
    "# restor graph from last checkpoint\n",
    "saver = tf.train.Saver(tf.global_variables()) # Saver\n",
    "session.run([\n",
    "    tf.local_variables_initializer(),\n",
    "    tf.global_variables_initializer(),\n",
    "])\n",
    "\n",
    "logger.info(\"Vocabulary Size: %i, max. sequence length(enc): %i\"%(VOCABULARY.size(), MAX_ENC_SEQ_LENGTH))\n",
    "logger.info(\"Trainings examples: %i,  batch size: %i\"%(NUM_TRAININGS_SEQUENCES, BATCH_SIZE))\n",
    "logger.info(\"Epochs: %i, batches per epoch:%i\\n\"%(NUM_EPOCHS, STEPS_PER_EPOCH ))\n",
    "\n",
    "#session.graph.finalize() # prevent nodes beeing added to graph\n",
    "st = time.time() # timing\n",
    "current_step=0\n",
    "TOTAL_STEPS = NUM_EPOCHS*STEPS_PER_EPOCH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainingsloop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Started training... \n",
      "INFO:__main__:[Train]: Epoch (01/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~521.506m\n",
      "INFO:__main__:[Train]: Epoch (01/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~516.037m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 001 TRAIN]: Mean Epoch Loss: 0.17917, Mean Valid Perms: 13.2\n",
      "\n",
      "\n",
      "INFO:__main__:Saving checkpoint, current learningrate: 0.00950\n",
      "INFO:__main__:[Train]: Epoch (02/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~509.409m\n",
      "INFO:__main__:[Train]: Epoch (02/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~501.679m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 002 TRAIN]: Mean Epoch Loss: 0.13491, Mean Valid Perms: 1.3\n",
      "\n",
      "\n",
      "INFO:__main__:[Train]: Epoch (03/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~493.652m\n",
      "INFO:__main__:[Train]: Epoch (03/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~485.461m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 003 TRAIN]: Mean Epoch Loss: 0.13788, Mean Valid Perms: 0.8\n",
      "\n",
      "\n",
      "INFO:__main__:[Train]: Epoch (04/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~477.301m\n",
      "INFO:__main__:[Train]: Epoch (04/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~468.721m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 004 TRAIN]: Mean Epoch Loss: 0.11111, Mean Valid Perms: 0.6\n",
      "\n",
      "\n",
      "INFO:__main__:[Train]: Epoch (05/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~460.099m\n",
      "INFO:__main__:[Train]: Epoch (05/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~451.408m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 005 TRAIN]: Mean Epoch Loss: 0.14084, Mean Valid Perms: 0.7\n",
      "\n",
      "\n",
      "INFO:__main__:[Train]: Epoch (06/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~442.748m\n",
      "INFO:__main__:[Train]: Epoch (06/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~433.950m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 006 TRAIN]: Mean Epoch Loss: 0.12842, Mean Valid Perms: 0.1\n",
      "\n",
      "\n",
      "INFO:__main__:Saving checkpoint, current learningrate: 0.00735\n",
      "INFO:__main__:[Train]: Epoch (07/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~425.106m\n",
      "INFO:__main__:[Train]: Epoch (07/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~416.260m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 007 TRAIN]: Mean Epoch Loss: 0.13061, Mean Valid Perms: 0.2\n",
      "\n",
      "\n",
      "INFO:__main__:[Train]: Epoch (08/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~407.378m\n",
      "INFO:__main__:[Train]: Epoch (08/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~398.490m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 008 TRAIN]: Mean Epoch Loss: 0.12687, Mean Valid Perms: 0.1\n",
      "\n",
      "\n",
      "INFO:__main__:[Train]: Epoch (09/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~389.581m\n",
      "INFO:__main__:[Train]: Epoch (09/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~380.665m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 009 TRAIN]: Mean Epoch Loss: 0.17314, Mean Valid Perms: 0.4\n",
      "\n",
      "\n",
      "INFO:__main__:[Train]: Epoch (10/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~371.718m\n",
      "INFO:__main__:[Train]: Epoch (10/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~362.711m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 010 TRAIN]: Mean Epoch Loss: 0.13419, Mean Valid Perms: 0.5\n",
      "\n",
      "\n",
      "INFO:__main__:[Train]: Epoch (11/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~353.709m\n",
      "INFO:__main__:[Train]: Epoch (11/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~344.692m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 011 TRAIN]: Mean Epoch Loss: 0.16213, Mean Valid Perms: 0.8\n",
      "\n",
      "\n",
      "INFO:__main__:Saving checkpoint, current learningrate: 0.00569\n",
      "INFO:__main__:Early stopping reached.\n",
      "INFO:__main__:[Train]: Epoch (12/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~335.677m\n",
      "INFO:__main__:[Train]: Epoch (12/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~326.653m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 012 TRAIN]: Mean Epoch Loss: 0.16764, Mean Valid Perms: 0.4\n",
      "\n",
      "\n",
      "INFO:__main__:Early stopping reached.\n",
      "INFO:__main__:[Train]: Epoch (13/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~317.617m\n",
      "INFO:__main__:[Train]: Epoch (13/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~308.647m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 013 TRAIN]: Mean Epoch Loss: 0.13360, Mean Valid Perms: 0.2\n",
      "\n",
      "\n",
      "INFO:__main__:Early stopping reached.\n",
      "INFO:__main__:[Train]: Epoch (14/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~299.667m\n",
      "INFO:__main__:[Train]: Epoch (14/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~290.671m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 014 TRAIN]: Mean Epoch Loss: 0.15468, Mean Valid Perms: 0.4\n",
      "\n",
      "\n",
      "INFO:__main__:Early stopping reached.\n",
      "INFO:__main__:[Train]: Epoch (15/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~281.655m\n",
      "INFO:__main__:[Train]: Epoch (15/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~272.866m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 015 TRAIN]: Mean Epoch Loss: 0.14603, Mean Valid Perms: 0.3\n",
      "\n",
      "\n",
      "INFO:__main__:Early stopping reached.\n",
      "INFO:__main__:[Train]: Epoch (16/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~263.977m\n",
      "INFO:__main__:[Train]: Epoch (16/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~256.370m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 016 TRAIN]: Mean Epoch Loss: 0.15088, Mean Valid Perms: 0.1\n",
      "\n",
      "\n",
      "INFO:__main__:Saving checkpoint, current learningrate: 0.00440\n",
      "INFO:__main__:Early stopping reached.\n",
      "INFO:__main__:[Train]: Epoch (17/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~247.579m\n",
      "INFO:__main__:[Train]: Epoch (17/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~238.462m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 017 TRAIN]: Mean Epoch Loss: 0.15147, Mean Valid Perms: 0.4\n",
      "\n",
      "\n",
      "INFO:__main__:Early stopping reached.\n",
      "INFO:__main__:[Train]: Epoch (18/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~229.340m\n",
      "INFO:__main__:[Train]: Epoch (18/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~220.254m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 018 TRAIN]: Mean Epoch Loss: 0.11487, Mean Valid Perms: 0.2\n",
      "\n",
      "\n",
      "INFO:__main__:Early stopping reached.\n",
      "INFO:__main__:[Train]: Epoch (19/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~211.099m\n",
      "INFO:__main__:[Train]: Epoch (19/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~201.949m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 019 TRAIN]: Mean Epoch Loss: 0.16449, Mean Valid Perms: 0.2\n",
      "\n",
      "\n",
      "INFO:__main__:Early stopping reached.\n",
      "INFO:__main__:[Train]: Epoch (20/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~192.791m\n",
      "INFO:__main__:[Train]: Epoch (22/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~156.154m\n",
      "INFO:__main__:[Train]: Epoch (22/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~146.994m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 022 TRAIN]: Mean Epoch Loss: 0.16448, Mean Valid Perms: 0.4\n",
      "\n",
      "\n",
      "INFO:__main__:Early stopping reached.\n",
      "INFO:__main__:[Train]: Epoch (23/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~137.828m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:[Train]: Epoch (23/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~128.659m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 023 TRAIN]: Mean Epoch Loss: 0.12464, Mean Valid Perms: 0.3\n",
      "\n",
      "\n",
      "INFO:__main__:Early stopping reached.\n",
      "INFO:__main__:[Train]: Epoch (24/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~119.481m\n",
      "INFO:__main__:[Train]: Epoch (24/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~110.306m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 024 TRAIN]: Mean Epoch Loss: 0.12434, Mean Valid Perms: 0.4\n",
      "\n",
      "\n",
      "INFO:__main__:Early stopping reached.\n",
      "INFO:__main__:[Train]: Epoch (25/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~101.126m\n",
      "INFO:__main__:[Train]: Epoch (25/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~91.940m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 025 TRAIN]: Mean Epoch Loss: 0.14951, Mean Valid Perms: 0.3\n",
      "\n",
      "\n",
      "INFO:__main__:Early stopping reached.\n",
      "INFO:__main__:[Train]: Epoch (26/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~82.754m\n",
      "INFO:__main__:[Train]: Epoch (26/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~73.566m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 026 TRAIN]: Mean Epoch Loss: 0.17732, Mean Valid Perms: 0.5\n",
      "\n",
      "\n",
      "INFO:__main__:Saving checkpoint, current learningrate: 0.00264\n",
      "INFO:__main__:Early stopping reached.\n",
      "INFO:__main__:[Train]: Epoch (27/30), Batch: (2137/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~64.375m\n",
      "INFO:__main__:[Train]: Epoch (27/30), Batch: (4274/4274)\n",
      "INFO:__main__:[Train]: Total loss: 0.00000, ETA: ~55.180m\n",
      "INFO:__main__:###################################\n",
      "INFO:__main__:[EPOCH 027 TRAIN]: Mean Epoch Loss: 0.13302, Mean Valid Perms: 0.1\n",
      "\n",
      "\n",
      "INFO:__main__:Early stopping reached.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Started training... \")\n",
    "early_stopping=0\n",
    "for current_epoch in range(0,NUM_EPOCHS):\n",
    "    # shuffle trainings data\n",
    "    x_permutation = np.random.permutation(ENCODER_INPUTS_TRAIN.shape[0])\n",
    "    epoch_losses = []\n",
    "    epoch_valid_perms = []\n",
    "    \n",
    "    for current_batch in range(0, STEPS_PER_EPOCH):\n",
    "        current_step = session.run(TF_GLOBAL_STEP) # will be updated by adam\n",
    "\n",
    "        # get current batch\n",
    "        feed_dict  = get_batch( # expects unshuffled inputs\n",
    "            graph_input_nodes,\n",
    "            x_permutation,\n",
    "            batch_index=current_batch, \n",
    "            enc_inputs=ENCODER_INPUTS_TRAIN, enc_seq_length=ENCODER_SEQLENGTH_TRAIN, \n",
    "            batch_size=BATCH_SIZE, \n",
    "            labeled_sig_by_example_id=labeled_signatures_by_example_id_TRAIN, labeled_examples_by_signature_id=labeled_examples_by_sig_id_TRAIN,  # this needs to be for trainings data as well\n",
    "            add_same_class_examples=ADD_EXTRA_POSITIVE_EXAMPLES_PER_CLASS, \n",
    "            print_shapes=False\n",
    "        )\n",
    "        \n",
    "        # run trainings operation\n",
    "        _, summaries, total_loss, valid_perms = session.run(fetches=[\n",
    "                training_step_op,\n",
    "                all_summaries_op,\n",
    "                total_loss_op,\n",
    "                num_non_zero_perms_op, \n",
    "            ],\n",
    "            feed_dict=feed_dict\n",
    "        )\n",
    "        # write summaries and metadata info to graph\n",
    "        summary_writer.add_summary(summaries, current_step)\n",
    "        \n",
    "        epoch_valid_perms.append(valid_perms)\n",
    "        \n",
    "        # print training progress every now and then\n",
    "        at_tenth_of_an_epoch = (current_step+1)%( max(1,int(STEPS_PER_EPOCH/2)) )==0\n",
    "        if at_tenth_of_an_epoch:\n",
    "            time_dif_in_s =  (time.time()-st)\n",
    "            time_per_step = time_dif_in_s/current_step\n",
    "            steps_left = TOTAL_STEPS-current_step\n",
    "            eta = steps_left*time_per_step/60\n",
    "            \n",
    "            logger.info(\"[Train]: Epoch (%0.2d/%0.2d), Batch: (%0.3d/%0.3d)\"%(current_epoch+1, NUM_EPOCHS, current_batch+1,STEPS_PER_EPOCH))\n",
    "            logger.info(\"[Train]: Total loss: %0.5f, ETA: ~%0.3fm\"%(total_loss, eta))\n",
    "\n",
    "        if total_loss>0.01: # only consider losses where valid triplets have been found\n",
    "            epoch_losses.append(total_loss)\n",
    "    \n",
    "    logger.info(\"###################################\")        \n",
    "    logger.info(\"[EPOCH %0.3d TRAIN]: Mean Epoch Loss: %0.5f, Mean Valid Perms: %0.1f\\n\\n\"%(current_epoch+1, np.mean(epoch_losses), np.mean(epoch_valid_perms) ) )\n",
    "    \n",
    "    if current_epoch%5==0:\n",
    "        logger.info (\"Saving checkpoint, current learningrate: %0.5f\"%session.run(tf_learning_rate))\n",
    "        saver.save(session, jp(GRAPH_DIR, MODEL_NAME), global_step=TF_GLOBAL_STEP)\n",
    "\n",
    "    if np.mean(epoch_valid_perms)<10:\n",
    "        early_stopping+=1\n",
    "        \n",
    "    if early_stopping>=10:\n",
    "        logger.info(\"Early stopping reached.\")\n",
    "        #break\n",
    "    \n",
    "et = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "print(\"Time: %0.3f\"%( (et-st)/60 ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode log lines\n",
    "\n",
    "* Encode all log lines with the final learned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "n_test_examples = ENCODER_INPUTS_TEST.shape[0]\n",
    "batch_size = 100\n",
    "num_embedding_steps = n_test_examples // batch_size +1\n",
    "EMBEDDED_TEST_INPUT = np.memmap(filename=jp(DATA_DIR, LOG_NAME+\"_test_embedded.mm\"), shape=(n_test_examples,STATE_SIZE), dtype=\"float32\", mode=\"w+\")\n",
    "\n",
    "logger.info(\"encoding %i test examples\"%n_test_examples)\n",
    "for i in range(num_embedding_steps):\n",
    "    if (i+1)%10==0:\n",
    "        print(\"Step %i/%i\"%(i,num_embedding_steps))\n",
    "    start_index = i * batch_size\n",
    "    end_index = min((i+1)*batch_size, n_test_examples )\n",
    "    \n",
    "    batch_enc_inp = ENCODER_INPUTS_TEST[start_index:end_index, :]\n",
    "    batch_seq_len = ENCODER_SEQLENGTH_TEST[start_index:end_index]\n",
    "    \n",
    "    feed_dict = {x_jd:batch_enc_inp, x_jd_seq_lengths:batch_seq_len, TF_KEEP_PROBABILTIY:1.0}\n",
    "    EMBEDDED_TEST_INPUT[start_index:end_index,:] = session.run(z_jd, feed_dict=feed_dict ) \n",
    "\n",
    "\n",
    "logger.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation  \n",
    "\n",
    "* http://www.johnmyleswhite.com/notebook/2014/03/24/a-note-on-the-johnson-lindenstrauss-lemma/\n",
    "* https://stackoverflow.com/questions/25104733/how-to-efficiently-calculate-huge-matrix-multiplication-tfidf-features-in-pyth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Validation rate (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# %load ./includes/validation_rate.py\n",
    "def evaluate_shard(out_csv_name, pw_ji, labels_x, labels_y, d = 0.00, d_step = 0.005, d_max=1.0):\n",
    "    \n",
    "    h.save_to_csv(data_rows=[[\n",
    "        \"Distance Threshhold\",\n",
    "        \"True Positives\", \n",
    "        \"False Positives\", \n",
    "        \"True Negative\", \n",
    "        \"False Negative\", \n",
    "        \"Num True Same\", \n",
    "        \"Num True Diff\", \n",
    "    ]], outfile_name=out_csv_name, mode=\"w\")\n",
    "    \n",
    "    \n",
    "    # calculate true accepts / false accepts based on labels\n",
    "    n_labels = len(labels_x)\n",
    "    tl_row = np.repeat( np.array(labels_x).reshape((n_labels,1)), n_labels, axis=1 )\n",
    "    tl_col = np.repeat( np.array(labels_y).reshape((1,n_labels)), n_labels, axis=0 ) \n",
    "    p_same = np.equal(tl_row, tl_col).astype(\"int8\")\n",
    "    p_diff = np.not_equal(tl_row, tl_col).astype(\"int8\")\n",
    "    num_true_same = p_same.sum()\n",
    "    num_true_diff = p_diff.sum()\n",
    "    \n",
    "    while True:\n",
    "        calc_same = np.zeros((n_labels, n_labels))\n",
    "        calc_same[np.where(pw_ji<=d)]=1\n",
    "        \n",
    "        tp = np.sum(np.logical_and(calc_same, p_same))\n",
    "        fp = np.sum(np.logical_and(calc_same, np.logical_not(p_same)))\n",
    "        tn = np.sum(np.logical_and(np.logical_not(calc_same), np.logical_not(p_same)))\n",
    "        fn = np.sum(np.logical_and(np.logical_not(calc_same), p_same))\n",
    "        \n",
    "        h.save_to_csv(data_rows=[[d, tp, fp, tn, fn,num_true_same,num_true_diff]], outfile_name=out_csv_name, mode=\"a\")\n",
    "        \n",
    "        d+=d_step\n",
    "        if d>d_max:\n",
    "            break\n",
    "\n",
    "def evaluate_all_shards(inputs, labels, shard_size,shard_indizes,  results_fn, d_start=0.0, d_step=0.005, d_max=1.0 ):\n",
    "    num_test_examples = inputs.shape[0]\n",
    "    for shard_index in shard_indizes:\n",
    "        shard_x, shard_y = shard_index\n",
    "        print(\"Current shard\", shard_index)\n",
    "        start_index_x = shard_x*shard_size\n",
    "        start_index_y = shard_y*shard_size\n",
    "        end_index_x = min((shard_x+1)*shard_size, num_test_examples)\n",
    "        end_index_y = min((shard_y+1)*shard_size, num_test_examples)\n",
    "\n",
    "        # calcualte pairwise distances\n",
    "        shard_inputs_x = inputs[start_index_x:end_index_x,:]\n",
    "        shard_labels_x = labels[start_index_x:end_index_x]\n",
    "\n",
    "        shard_inputs_y = inputs[start_index_y:end_index_y,:]\n",
    "        shard_labels_y = labels[start_index_y:end_index_y]\n",
    "\n",
    "        pw_ji = pairwise_distances(shard_inputs_x,shard_inputs_y, metric=\"euclidean\", n_jobs=8) \n",
    "\n",
    "        # evaluate pairwise distances \n",
    "        out_csv_name = results_fn+\"_%0.2d-%0.2d\"%(shard_x, shard_y)\n",
    "        evaluate_shard(out_csv_name, pw_ji, shard_labels_x, shard_labels_y, d=d_start,  d_step = d_step, d_max=d_max)            \n",
    "            \n",
    "def run_evaluation(inputs, labels, shard_size, results_fn, d_start=0.0, d_step=0.005, d_max=1.0):\n",
    "    results_fn = results_fn%shard_size\n",
    "    \n",
    "    num_test_examples = inputs.shape[0]\n",
    "    num_x = inputs.shape[0]//shard_size\n",
    "    if not num_test_examples%shard_size==0 :# need to be a square matrix\n",
    "        print(\"Allowed shard sizes\")\n",
    "        for i in range(100, num_test_examples):\n",
    "            if num_test_examples%i==0:\n",
    "                print(i)\n",
    "        0/0\n",
    "    shard_indizes = list(itertools.product(range(num_x),repeat=2))\n",
    "    num_shards = len(shard_indizes)\n",
    "    num_distances = len(list(np.arange(d_start,d_max,d_step)))\n",
    "    num_metrics = 7 \n",
    "    \n",
    "    evaluate_all_shards(inputs, labels, shard_size, shard_indizes, results_fn, d_start, d_step, d_max )\n",
    "    \n",
    "    all_data = np.ndarray(shape=(num_shards, num_distances, num_metrics), dtype=\"float32\")\n",
    "\n",
    "    for i, shard_index in enumerate(shard_indizes):\n",
    "        # load shard\n",
    "        shard_x, shard_y = shard_index\n",
    "        out_csv_name = results_fn+\"_%0.2d-%0.2d\"%(shard_x, shard_y)\n",
    "        shard_data = h.load_from_csv(out_csv_name)\n",
    "        shard_data = shard_data[1:] # cut header row \n",
    "        all_data[i] = np.array(shard_data)\n",
    "\n",
    "\n",
    "    final_data  = np.ndarray(shape=(num_distances, 10), dtype=\"float32\")\n",
    "\n",
    "    final_data[:,0] = all_data[0,:,0] # all distances (are same over all shards)\n",
    "\n",
    "    final_data[:,1] = all_data.sum(axis=0)[:,1] # True Positives\n",
    "    final_data[:,2] = all_data.sum(axis=0)[:,2] # False Positives\n",
    "    final_data[:,3] = all_data.sum(axis=0)[:,3] # True Negatives\n",
    "    final_data[:,4] = all_data.sum(axis=0)[:,4] # False Negatives\n",
    "    final_data[:,5] = all_data.sum(axis=0)[:,5] # Num true same (are same over all shards)\n",
    "    final_data[:,6] = all_data.sum(axis=0)[:,6] # Num true diff  (are same over all shards)\n",
    "\n",
    "    final_data[:,7] = final_data[:,1]/final_data[:,5] # validation rate \n",
    "    final_data[:,8] = final_data[:,2]/final_data[:,6] # false acceptance rate  \n",
    "\n",
    "    final_data[:,9] = (final_data[:,1] + final_data[:,3]) / (final_data[:,1:1+4].sum(axis=1)) \n",
    "\n",
    "    \n",
    "    h.save_to_csv(data_rows=[[\n",
    "            \"Distance Threshhold\",\n",
    "            \"True Positives\", \n",
    "            \"False Positives\", \n",
    "            \"True Negative\", \n",
    "            \"False Negative\", \n",
    "            \"Num true same\", \n",
    "            \"Num true diff\", \n",
    "            \"Validation Rate\",\n",
    "            \"False Acceptance Rate\",\n",
    "            \"Accuracy\"\n",
    "        ]], outfile_name=results_fn, mode=\"w\", convert_float=False)\n",
    "    h.save_to_csv(data_rows=final_data, outfile_name=results_fn, mode=\"a\", convert_float=True)\n",
    "\n",
    "    logger.info(\"Evaluation done, saved to '%s'\"%results_fn)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shards_9460 = run_evaluation(inputs=EMBEDDED_TEST_INPUT, labels=LABELS_TEST, shard_size=SHARD_SIZE, results_fn=RESULTS_FILE, d_step=0.01, d_max=2.0)\n",
    "# bgl2 9460\n",
    "# spirit2 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
